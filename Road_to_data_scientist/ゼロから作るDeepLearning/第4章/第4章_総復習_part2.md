# 📊 第4章 総復習 Part 2：実践編

## 7. 学習アルゴリズムの全体フロー

### 7.1 MNIST学習の完全フロー

```
┌─────────────────────────────────────────────────────┐
│  ニューラルネットワーク学習の完全フロー（MNIST例）     │
└─────────────────────────────────────────────────────┘

【準備フェーズ】
┌────────────────────────────────┐
│ 1. データの読み込みと前処理     │
│    from dataset.mnist import    │
│        load_mnist               │
│                                 │
│    (x_train, t_train),          │
│    (x_test, t_test) =           │
│        load_mnist(              │
│            normalize=True,      │
│            one_hot_label=True)  │
│                                 │
│    x_train: (60000, 784)        │
│    t_train: (60000, 10)         │
│    x_test: (10000, 784)         │
│    t_test: (10000, 10)          │
└────────────────────────────────┘
         ↓
┌────────────────────────────────┐
│ 2. ネットワークの初期化         │
│    network = TwoLayerNet(       │
│        input_size=784,          │
│        hidden_size=50,          │
│        output_size=10)          │
│                                 │
│    構造:                        │
│    784 (28×28) → 50 → 10       │
│                                 │
│    パラメータ数:                │
│    W1: 784×50 = 39,200         │
│    b1: 50                       │
│    W2: 50×10 = 500              │
│    b2: 10                       │
│    合計: 39,760個               │
└────────────────────────────────┘
         ↓
┌────────────────────────────────┐
│ 3. ハイパーパラメータ設定       │
│    iters_num = 10,000           │
│    batch_size = 100             │
│    learning_rate = 0.1          │
│                                 │
│    train_size = 60,000          │
│    iter_per_epoch =             │
│        train_size / batch_size  │
│        = 600                    │
│                                 │
│    エポック数 =                 │
│        iters_num / iter_per_epoch│
│        = 10,000 / 600 ≈ 16.7   │
└────────────────────────────────┘

【学習ループ】
         ↓
╔════════════════════════════════════════════════════╗
║ for i in range(10,000):  # 10,000回の更新          ║
║                                                    ║
║  ┌──────────────────────────────────────────┐   ║
║  │ ステップ1: ミニバッチ抽出                │   ║
║  │                                          │   ║
║  │ # ランダムに100個のインデックス選択      │   ║
║  │ batch_mask = np.random.choice(           │   ║
║  │     train_size, batch_size)              │   ║
║  │                                          │   ║
║  │ # 該当データを取得                       │   ║
║  │ x_batch = x_train[batch_mask]            │   ║
║  │ # 形状: (100, 784)                       │   ║
║  │                                          │   ║
║  │ t_batch = t_train[batch_mask]            │   ║
║  │ # 形状: (100, 10)                        │   ║
║  └──────────────────────────────────────────┘   ║
║         ↓                                          ║
║  ┌──────────────────────────────────────────┐   ║
║  │ ステップ2: 順伝播（Forward Propagation）  │   ║
║  │                                          │   ║
║  │ # 第1層                                  │   ║
║  │ a1 = x_batch @ W1 + b1                   │   ║
║  │ # 形状: (100, 784) @ (784, 50) = (100,50)│   ║
║  │                                          │   ║
║  │ z1 = sigmoid(a1)                         │   ║
║  │ # 活性化: (100, 50)                      │   ║
║  │                                          │   ║
║  │ # 第2層                                  │   ║
║  │ a2 = z1 @ W2 + b2                        │   ║
║  │ # 形状: (100, 50) @ (50, 10) = (100, 10)│   ║
║  │                                          │   ║
║  │ y = softmax(a2)                          │   ║
║  │ # 確率化: (100, 10)                      │   ║
║  │ # 各行の合計 = 1.0                       │   ║
║  └──────────────────────────────────────────┘   ║
║         ↓                                          ║
║  ┌──────────────────────────────────────────┐   ║
║  │ ステップ3: 損失計算                       │   ║
║  │                                          │   ║
║  │ loss = cross_entropy_error(y, t_batch)   │   ║
║  │                                          │   ║
║  │ # 計算内容:                              │   ║
║  │ # 各サンプルの損失を計算                 │   ║
║  │ # 100個の平均を取る                      │   ║
║  │                                          │   ║
║  │ loss_list.append(loss)                   │   ║
║  │ # 記録（後でグラフ化）                   │   ║
║  └──────────────────────────────────────────┘   ║
║         ↓                                          ║
║  ┌──────────────────────────────────────────┐   ║
║  │ ステップ4: 勾配計算                       │   ║
║  │                                          │   ║
║  │ # 数値微分版（遅い）                     │   ║
║  │ grad = network.numerical_gradient(       │   ║
║  │     x_batch, t_batch)                    │   ║
║  │                                          │   ║
║  │ # または誤差逆伝播法版（速い）           │   ║
║  │ # grad = network.gradient(               │   ║
║  │ #     x_batch, t_batch)                  │   ║
║  │                                          │   ║
║  │ # 結果: 辞書形式                         │   ║
║  │ # grad = {                               │   ║
║  │ #     'W1': (784, 50),                   │   ║
║  │ #     'b1': (50,),                       │   ║
║  │ #     'W2': (50, 10),                    │   ║
║  │ #     'b2': (10,)                        │   ║
║  │ # }                                      │   ║
║  └──────────────────────────────────────────┘   ║
║         ↓                                          ║
║  ┌──────────────────────────────────────────┐   ║
║  │ ステップ5: パラメータ更新                 │   ║
║  │                                          │   ║
║  │ for key in ('W1', 'b1', 'W2', 'b2'):    │   ║
║  │     network.params[key] -=               │   ║
║  │         learning_rate * grad[key]        │   ║
║  │                                          │   ║
║  │ # 具体的には:                            │   ║
║  │ # W1 ← W1 - 0.1 × ∂L/∂W1                │   ║
║  │ # b1 ← b1 - 0.1 × ∂L/∂b1                │   ║
║  │ # W2 ← W2 - 0.1 × ∂L/∂W2                │   ║
║  │ # b2 ← b2 - 0.1 × ∂L/∂b2                │   ║
║  └──────────────────────────────────────────┘   ║
║         ↓                                          ║
║  ┌──────────────────────────────────────────┐   ║
║  │ ステップ6: エポックごとに精度評価         │   ║
║  │                                          │   ║
║  │ if i % iter_per_epoch == 0:              │   ║
║  │     # 600回ごと（1エポックごと）         │   ║
║  │                                          │   ║
║  │     train_acc = network.accuracy(        │   ║
║  │         x_train, t_train)                │   ║
║  │     test_acc = network.accuracy(         │   ║
║  │         x_test, t_test)                  │   ║
║  │                                          │   ║
║  │     train_acc_list.append(train_acc)     │   ║
║  │     test_acc_list.append(test_acc)       │   ║
║  │                                          │   ║
║  │     print(f"Epoch {epoch}: "             │   ║
║  │           f"train={train_acc:.3f}, "     │   ║
║  │           f"test={test_acc:.3f}")        │   ║
║  └──────────────────────────────────────────┘   ║
║         ↓                                          ║
║  # 次のイテレーションへ                            ║
║  continue                                         ║
╚════════════════════════════════════════════════════╝

【学習結果の出力例】
         ↓
┌────────────────────────────────┐
│ Epoch  0: train=0.098, test=0.098│
│ Epoch  1: train=0.788, test=0.792│
│ Epoch  2: train=0.873, test=0.877│
│ Epoch  3: train=0.897, test=0.899│
│ Epoch  5: train=0.920, test=0.919│
│ Epoch 10: train=0.955, test=0.947│
│ Epoch 15: train=0.972, test=0.961│
│ Epoch 16: train=0.975, test=0.964│
└────────────────────────────────┘

【結果の可視化】
         ↓
┌────────────────────────────────┐
│ 損失の推移                      │
│                                 │
│ 損失 ↑                          │
│ 2.5 │╲                           │
│     │ ╲___                       │
│ 1.5 │     ────___                │
│     │            ────____        │
│ 0.5 │                   ────__   │
│     │                         ───│
│ 0.0 └──────────────────────────→│
│      0   2,500  5,000    10,000 │
│            イテレーション         │
└────────────────────────────────┘
         ↓
┌────────────────────────────────┐
│ 精度の推移                      │
│                                 │
│ 精度 ↑                          │
│100% │                   ______  │
│     │               ____         │
│ 80% │          ____              │
│     │      ___                   │
│ 60% │  __                        │
│     │                            │
│  0% └──────────────────────────→│
│      0    5    10   15 (エポック)│
│                                 │
│ 最終結果:                       │
│ 訓練精度: 97.5%                 │
│ テスト精度: 96.4%               │
│                                 │
│ → 良い汎化性能！                │
└────────────────────────────────┘
```

### 7.2 実装の重要ポイント

```python
【完全な実装例】

import sys, os
sys.path.append(os.pardir)
import numpy as np
from dataset.mnist import load_mnist
from two_layer_net import TwoLayerNet

# ============================================
# 1. データの読み込み
# ============================================
(x_train, t_train), (x_test, t_test) = \
    load_mnist(normalize=True, one_hot_label=True)

# データ形状の確認
print(f"x_train: {x_train.shape}")  # (60000, 784)
print(f"t_train: {t_train.shape}")  # (60000, 10)

# ============================================
# 2. ネットワークの初期化
# ============================================
network = TwoLayerNet(
    input_size=784,
    hidden_size=50,
    output_size=10
)

# ============================================
# 3. ハイパーパラメータの設定
# ============================================
iters_num = 10000        # 更新回数
batch_size = 100         # バッチサイズ
learning_rate = 0.1      # 学習率

train_size = x_train.shape[0]
iter_per_epoch = max(train_size // batch_size, 1)

# ============================================
# 4. 記録用のリスト
# ============================================
train_loss_list = []
train_acc_list = []
test_acc_list = []

# ============================================
# 5. 学習ループ
# ============================================
for i in range(iters_num):
    # ミニバッチの取得
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]
    
    # 勾配の計算
    # grad = network.numerical_gradient(x_batch, t_batch)  # 遅い
    grad = network.gradient(x_batch, t_batch)  # 速い（誤差逆伝播）
    
    # パラメータの更新
    for key in ('W1', 'b1', 'W2', 'b2'):
        network.params[key] -= learning_rate * grad[key]
    
    # 損失の記録
    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)
    
    # 1エポックごとに精度を計算
    if i % iter_per_epoch == 0:
        epoch = i // iter_per_epoch
        train_acc = network.accuracy(x_train, t_train)
        test_acc = network.accuracy(x_test, t_test)
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)
        print(f"Epoch {epoch:2d}: train={train_acc:.4f}, test={test_acc:.4f}")

# ============================================
# 6. 結果の可視化
# ============================================
import matplotlib.pyplot as plt

# 損失の推移
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(train_loss_list)
plt.xlabel('Iterations')
plt.ylabel('Loss')
plt.title('Training Loss')
plt.grid(True)

# 精度の推移
plt.subplot(1, 2, 2)
epochs = np.arange(len(train_acc_list))
plt.plot(epochs, train_acc_list, label='Train Accuracy', marker='o')
plt.plot(epochs, test_acc_list, label='Test Accuracy', marker='s')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Accuracy')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

print(f"\n最終結果:")
print(f"訓練精度: {train_acc_list[-1]:.4f}")
print(f"テスト精度: {test_acc_list[-1]:.4f}")
```

---

## 8. 重要な数式のまとめ

```
┌──────────────────────────────────────────────┐
│         第4章で登場した重要な数式              │
└──────────────────────────────────────────────┘

【1. 二乗和誤差（MSE）】

E = 1/2 Σₖ (yₖ - tₖ)²

各項の意味:
  E    : 誤差（Error）
  yₖ   : k番目の出力（予測値）
  tₖ   : k番目の正解ラベル
  Σₖ   : kについての総和
  1/2  : 微分を簡単にするための係数

用途: 回帰問題
例  : 株価予測、気温予測

微分:
  ∂E/∂yₖ = yₖ - tₖ  ← シンプル！

────────────────────────────────────────────

【2. 交差エントロピー誤差（CEE）】

E = -Σₖ tₖ log(yₖ)

各項の意味:
  E    : 誤差（Error）
  yₖ   : k番目の出力確率（softmax後）
  tₖ   : k番目の正解ラベル（one-hot）
  log  : 自然対数（底e）
  -    : 負の符号（正の値にする）

tがone-hotの場合:
  E = -log(yc)  ← 正解クラスcだけ

用途: 分類問題
例  : 画像分類、音声認識

微分（softmaxと組み合わせ）:
  ∂E/∂aₖ = yₖ - tₖ  ← MSEと同じ形！

────────────────────────────────────────────

【3. ミニバッチ版交差エントロピー】

E = -1/N Σₙ Σₖ tₙₖ log(yₙₖ)

各項の意味:
  N    : バッチサイズ
  n    : n番目のサンプル
  k    : k番目のクラス

整数ラベル版:
  E = -1/N Σₙ log(yₙ,tₙ)

意味: N個のデータの平均損失

実装:
```python
def cross_entropy_error(y, t):
    if y.ndim == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)
    
    batch_size = y.shape[0]
    return -np.sum(t * np.log(y + 1e-7)) / batch_size
```

────────────────────────────────────────────

【4. 微分の定義】

f'(x) = lim[h→0] (f(x+h) - f(x)) / h

各項の意味:
  f'(x) : xにおける微分（導関数）
  lim   : 極限
  h     : 微小な変化量
  f(x+h): xをhだけ増やしたときの関数値

前方差分（精度低い）:
  f'(x) ≈ (f(x+h) - f(x)) / h

中心差分（精度高い）★:
  f'(x) ≈ (f(x+h) - f(x-h)) / (2h)

実装で使用: h = 10⁻⁴ = 0.0001

────────────────────────────────────────────

【5. 偏微分】

f(x₀, x₁, ..., xₙ) に対して:

∂f/∂xᵢ = lim[h→0] (f(..., xᵢ+h, ...) - f(..., xᵢ, ...)) / h

意味:
  他の変数を固定して、xᵢだけ微分

例: f(x, y) = x² + xy + y²

  ∂f/∂x = 2x + y  ← yは定数として扱う
  ∂f/∂y = x + 2y  ← xは定数として扱う

────────────────────────────────────────────

【6. 勾配ベクトル】

∇f = (∂f/∂x₀, ∂f/∂x₁, ..., ∂f/∂xₙ)

各項の意味:
  ∇f    : ナブラf、勾配
  ∂f/∂xᵢ: xᵢ方向の偏微分

性質:
  1. ∇f の方向 = 関数が最も増加する方向
  2. -∇f の方向 = 関数が最も減少する方向
  3. ||∇f|| = 変化の急峻さ

例: f(x, y) = x² + y²

  ∇f = (2x, 2y)
  
  点(3, 4)での勾配:
    ∇f(3, 4) = (6, 8)
    
  大きさ:
    ||∇f|| = √(6² + 8²) = 10

────────────────────────────────────────────

【7. 勾配降下法の更新式】

W ← W - η ∂L/∂W
b ← b - η ∂L/∂b

各項の意味:
  W     : 重み行列
  b     : バイアスベクトル
  η     : 学習率（イータ、eta）
  ∂L/∂W : 損失Lの重みWに対する勾配
  ∂L/∂b : 損失Lのバイアスbに対する勾配
  ←    : 代入（更新）

ベクトル表記:
  θ ← θ - η∇θL
  
  θ: すべてのパラメータ

意味:
  損失が減る方向（勾配の逆方向）に
  学習率の分だけパラメータを移動

────────────────────────────────────────────

【8. 連鎖律（Chain Rule）】

z = f(y), y = g(x) のとき:

∂z/∂x = (∂z/∂y) × (∂y/∂x)

例: z = (2x + 3)²

  y = 2x + 3
  z = y²
  
  ∂z/∂y = 2y
  ∂y/∂x = 2
  
  ∂z/∂x = (∂z/∂y) × (∂y/∂x)
        = 2y × 2
        = 2(2x + 3) × 2
        = 4(2x + 3)

応用: 誤差逆伝播法の数学的基礎

────────────────────────────────────────────

【9. ソフトマックス関数】

yₖ = exp(aₖ) / Σⱼ exp(aⱼ)

各項の意味:
  yₖ   : k番目の出力確率
  aₖ   : k番目の入力（スコア）
  exp  : 指数関数
  Σⱼ   : すべてのクラスについての総和

性質:
  1. 0 ≤ yₖ ≤ 1  （確率）
  2. Σₖ yₖ = 1   （確率の総和=1）

オーバーフロー対策版:
  yₖ = exp(aₖ - C) / Σⱼ exp(aⱼ - C)
  
  C = max(a)  ← 最大値を引く

────────────────────────────────────────────

【10. シグモイド関数とその微分】

σ(x) = 1 / (1 + exp(-x))

微分:
  σ'(x) = σ(x) × (1 - σ(x))

便利な性質:
  出力yを使って微分が計算できる！
  
  y = σ(x) ならば
  ∂y/∂x = y(1 - y)

実装:
```python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_grad(x):
    return (1.0 - sigmoid(x)) * sigmoid(x)

# または出力を使う場合
def sigmoid_grad_from_output(y):
    return y * (1.0 - y)
```
```

---

## 9. よくある間違いと対処法

[内容は第4章_総復習.mdに記載済み]

---

## 10. 第4章チェックリスト

```
┌──────────────────────────────────────────────┐
│      理解度チェック：すべて答えられますか？      │
└──────────────────────────────────────────────┘

【基礎概念】

□ 学習とは何か、一言で説明できる
  答: 訓練データから最適な重みパラメータを見つけること

□ 損失関数の役割を3つ挙げられる
  答: ①性能を数値化 ②学習の指標 ③微分可能な目的関数

□ なぜ精度ではなく損失関数を使うか説明できる
  答: 精度は離散的で微分できない、損失は連続で微分可能

□ 訓練データとテストデータを分ける理由を説明できる
  答: 汎化性能の評価、過学習の検出

□ 過学習とは何か、兆候を2つ挙げられる
  答: 訓練データへの過剰適応
      兆候: ①訓練精度高、テスト精度低 ②損失のギャップ

【損失関数】

□ 二乗和誤差の数式を書ける
  答: E = (1/2)Σ(y-t)²

□ 交差エントロピー誤差の数式を書ける
  答: E = -Σt·log(y)

□ どちらをどの問題に使うか判断できる
  答: MSE→回帰、CEE→分類

□ one-hot表現を説明できる
  答: 正解クラスだけ1、他は0のベクトル表現

□ バッチサイズで割る理由を説明できる
  答: バッチサイズに依らない比較可能な値にするため

【ミニバッチ学習】

□ ミニバッチ学習の利点を3つ挙げられる
  答: ①計算が速い ②メモリ効率 ③汎化性能向上

□ 1エポックの意味を説明できる
  答: 全訓練データを1回学習し終えた単位

□ イテレーションとエポックの違いを説明できる
  答: イテレーション=更新1回、エポック=全データ1周

□ np.random.choiceの役割を説明できる
  答: ランダムサンプリングでミニバッチを作成

□ 適切なバッチサイズの範囲を答えられる
  答: 32〜256（一般的には128が推奨）

【微分と勾配】

□ 微分の直感的な意味を説明できる
  答: ある点での変化率（傾き）

□ 数値微分の原理を説明できる
  答: (f(x+h)-f(x-h))/(2h)で近似

□ なぜh=1e-4を使うか説明できる
  答: 丸め誤差と近似誤差のバランスが良い

□ 偏微分と勾配の違いを説明できる
  答: 偏微分=1変数の微分、勾配=全偏微分のベクトル

□ 勾配の方向が意味することを説明できる
  答: 関数が最も増加する方向

【勾配降下法】

□ 勾配降下法の更新式を書ける
  答: x ← x - η∇f(x)

□ 学習率の役割を説明できる
  答: 1回の更新での移動距離を決める

□ 学習率が大きすぎるとどうなるか説明できる
  答: 発散する（最小値を飛び越える）

□ 学習率が小さすぎるとどうなるか説明できる
  答: 学習が遅い（最小値に到達できない）

□ 適切な学習率の初期値を答えられる
  答: 0.01（その後、学習曲線を見て調整）

【学習アルゴリズム】

□ 学習の4ステップを順番に言える
  答: ①ミニバッチ ②順伝播 ③損失計算 ④勾配計算 ⑤更新

□ SGDの正式名称と意味を説明できる
  答: Stochastic Gradient Descent（確率的勾配降下法）
      ランダムなミニバッチで勾配を近似

□ ハイパーパラメータを5つ挙げられる
  答: ①学習率 ②バッチサイズ ③エポック数 ④隠れ層数 ⑤ニューロン数

□ 学習曲線から過学習を検出できる
  答: 訓練損失は減少するがテスト損失が増加

□ 数値微分と誤差逆伝播法の違いを説明できる
  答: 数値微分=遅いが簡単、誤差逆伝播=速いが複雑

【実装スキル】

□ 二乗和誤差をNumPyで実装できる
  ```python
  def mse(y, t):
      return 0.5 * np.sum((y - t)**2)
  ```

□ 交差エントロピー誤差を実装できる（バッチ対応）
  ```python
  def cee(y, t):
      batch_size = y.shape[0]
      return -np.sum(t * np.log(y + 1e-7)) / batch_size
  ```

□ 数値微分を実装できる
  ```python
  def numerical_gradient(f, x):
      h = 1e-4
      grad = np.zeros_like(x)
      for idx in range(x.size):
          tmp = x[idx]
          x[idx] = tmp + h
          fxh1 = f(x)
          x[idx] = tmp - h
          fxh2 = f(x)
          grad[idx] = (fxh1 - fxh2) / (2*h)
          x[idx] = tmp
      return grad
  ```

□ 勾配降下法を実装できる
  ```python
  def gradient_descent(f, init_x, lr=0.01, step_num=100):
      x = init_x.copy()
      for i in range(step_num):
          grad = numerical_gradient(f, x)
          x -= lr * grad
      return x
  ```

□ 2層ニューラルネットワークの学習ループを書ける
  （上記の完全な実装例を参照）

□ 学習曲線をmatplotlibで描画できる
  ```python
  plt.plot(loss_list)
  plt.xlabel('Iteration')
  plt.ylabel('Loss')
  plt.show()
  ```

────────────────────────────────────────────

【スコアリング】

✓の数:
  50-54: 完璧！第5章へGO
  45-49: ほぼ理解、復習推奨
  40-44: もう一度重要部分を
  40未満: じっくり復習を
```

---

## 🎯 第4章の最重要ポイント

### Top 10 重要概念

```
┌────────────────────────────────────────┐
│  第4章で絶対に押さえるべき10のポイント    │
└────────────────────────────────────────┘

1. 【学習=最適化】
   学習とは損失関数を最小化する重みを探すこと

2. 【損失関数の役割】
   性能の「悪さ」を微分可能な形で数値化

3. 【なぜ精度ではダメか】
   精度は離散的→微分不可、損失は連続的→微分可能

4. 【ミニバッチ学習】
   全データではなく一部で学習→高速化と汎化

5. 【勾配=方向指示器】
   勾配は損失が増える方向、逆方向で最小化

6. 【勾配降下法】
   x ← x - η∇f(x) で繰り返し更新

7. 【学習率の重要性】
   大→発散、小→遅い、適切な値が必須

8. 【数値微分 vs 誤差逆伝播】
   数値微分=教育用、誤差逆伝播=実用

9. 【過学習の検出】
   訓練精度↑、テスト精度↓ の時は危険信号

10. 【学習の4ステップ】
    ①ミニバッチ→②順伝播→③勾配計算→④更新
```

### 学習の本質（一枚絵）

```
┌─────────────────────────────────────────┐
│         ニューラルネットワーク学習         │
└─────────────────────────────────────────┘

  何を？   重みW、バイアスb
    ↓
  どうやって？  勾配降下法
    ↓
  何のため？   損失関数の最小化
    ↓
  目的は？   予測精度の向上
    ↓
  ゴールは？  汎化性能の獲得
    ↓
  実現手段   データから自動学習

【学習のループ】

データ → ミニバッチ → 予測 → 損失 → 勾配 → 更新
  ↑                                        ↓
  └────────────── 繰り返し ─────────────────┘

【学習前 vs 学習後】

学習前:
  重み: ランダム
  精度: 10%（10クラスなので運）
  損失: 2.3（大きい）

学習後（10,000回更新）:
  重み: 最適化済み
  精度: 97%（大幅向上！）
  損失: 0.1（小さい）
```

---

## 次章への準備

### 第5章で学ぶこと

```
┌─────────────────────────────────────────┐
│     第5章：誤差逆伝播法（Backpropagation）│
└─────────────────────────────────────────┘

【なぜ必要？】
  数値微分は遅すぎて実用不可
  → 高速な勾配計算手法が必要
  → 誤差逆伝播法！

【何を学ぶ？】
  1. 計算グラフ
     視覚的に計算を理解

  2. 連鎖律
     合成関数の微分規則

  3. 各層の逆伝播
     加算、乗算、ReLU、Affine...

  4. 実装
     Layerクラスとforward/backward

  5. 勾配チェック
     実装が正しいか確認

【数学的準備】
  連鎖律の理解が鍵！
  
  z = f(y), y = g(x) のとき:
  dz/dx = (dz/dy) × (dy/dx)

【実装イメージ】
  class Layer:
      def forward(self, x):
          # 順伝播
          return output
      
      def backward(self, dout):
          # 逆伝播
          return dx

【期待される成果】
  • 勾配計算が数千倍高速化
  • 深い層のネットワークが学習可能に
  • 実用的な深層学習の実現
```

---

## おめでとうございます！ 🎉

```
┌─────────────────────────────────────────┐
│        第4章の学習を完了しました！          │
└─────────────────────────────────────────┘

あなたは今、以下のことができるようになりました:

✓ ニューラルネットワークの学習原理を理解
✓ 損失関数の役割と使い分けを理解
✓ 勾配と勾配降下法を理解
✓ ミニバッチ学習を実装
✓ 2層ニューラルネットワークで学習
✓ 学習曲線を解釈
✓ 過学習を検出

次のステップ:
  → 第5章で誤差逆伝播法を学び、
    本格的な深層学習へ！

Let's keep learning! 🚀
```
