# ニューラルネットワークの学習

## 第4章の概要

この章では、ニューラルネットワークが**データから学習する仕組み**を理解します。第3章では事前学習済みの重みを使いましたが、ここでは**訓練データから最適な重みを自動的に見つける方法**を学びます。

第4章で学ぶこと：
- **損失関数**：学習の指標
- **数値微分**と**勾配**：パラメータの更新方向
- **勾配降下法**：最適化アルゴリズム
- **学習アルゴリズムの実装**

---

## 1. 学習とは

### ニューラルネットワークの特徴

**ニューラルネットワークの最大の特徴は「学習できる」こと**

従来の機械学習では、人間が手作業で特徴量を設計する必要がありました。例えば、画像認識なら「エッジ検出」「色ヒストグラム」などを人間が考えて実装していました。

**従来の機械学習の流れ：**
```
生データ → 特徴量抽出（人間が設計） → 機械学習 → 出力
例：画像 → エッジ、色、テクスチャ（人が設計） → SVM → 分類結果
```

**ディープラーニングの流れ：**
```
生データ → ニューラルネットワーク → 出力
例：画像 → [自動で特徴を学習] → 分類結果
        （特徴量も自動で学習）
```

**特徴量（Feature）とは？**
入力データから本質的なパターンを抽出する変換器のこと。

例：
- 手書き文字「8」を認識する場合
  - 従来：人間が「2つの円が縦に並んでいる」という特徴を設計
  - ディープラーニング：ネットワークが自動で「8らしさ」を学習

### 学習の目的

**訓練データから最適な重みパラメータを見つけること**

もっと具体的に言うと：
- **目的**：損失関数（モデルの誤差）を最小化する重みWとバイアスbを見つける
- **方法**：損失関数を指標として、勾配（傾き）を計算し、重みを少しずつ調整
- **結果**：訓練データの予測精度が向上し、未知データにも対応できる

**例え話：**
重みの調整は、目隠しをして山を下りるようなもの。
- 足元の傾き（勾配）を感じながら
- 少しずつ下る方向（損失が減る方向）に進む
- 最終的に谷底（損失が最小）に到達する

---

## 2. 訓練データとテストデータ

### データの分割

機械学習では、データを2つに分けます：

- **訓練データ（Training Data）**：学習に使用するデータ
- **テストデータ（Test Data）**：学習済みモデルの性能評価に使用

**重要：** テストデータは学習に使わない！

### なぜ分けるのか？

**汎化能力（Generalization Ability）を評価するため**

- **汎化能力**：未知のデータに対する認識能力
- 訓練データだけで評価すると、特定のデータにだけ最適化される危険性

### 過学習（Overfitting）

**過学習**：訓練データに過剰に適応し、汎化能力が低下すること

```
訓練データの精度：99%  ← 高い！
テストデータの精度：70%  ← 低い... 過学習！
```

**過学習の兆候：**
- 訓練データの精度は高いが、テストデータの精度が低い
- モデルが訓練データを「暗記」している状態

---

## 3. 損失関数（Loss Function）

### 損失関数とは

**損失関数**は、ニューラルネットワークの性能の「悪さ」を数値化した指標です。

もっと分かりやすく言うと：
- **損失が大きい** = モデルの予測が正解から大きくずれている（性能が悪い）
- **損失が小さい** = モデルの予測が正解に近い（性能が良い）

**学習の目標**：この損失関数の値をできるだけ小さくする重みパラメータを見つけること

**損失関数が学習の鍵となる指標です。**

### なぜ認識精度を使わないのか？

直感的には「正解率（認識精度）」を最大化したいところですが、実は**認識精度は学習に使えません**。

**理由：認識精度は微分できないため**

具体例で考えてみましょう：
```
100個のデータで33個正解（精度33%）
↓ 重みをわずかに変更
100個のデータで33個正解（精度33%）← 変化なし！
```

**問題点：**
- 認識精度は離散的な値（33%, 34%, 35%...）
- 重みをわずかに変えても、正解数が変わらなければ精度は変わらない
- 精度が変わらない = 微分が0 = どちらに重みを動かすべきかわからない

**解決策：損失関数を使う**
```
損失関数の値：1.234
↓ 重みをわずかに変更
損失関数の値：1.231 ← わずかに減少！
```

損失関数なら：
- 連続的に変化する（1.234 → 1.231 → 1.228...）
- 重みを少し変えれば、損失も必ず変化する
- 微分が意味を持つ = どちらに重みを動かすべきかわかる

**重要：** 重み更新には連続的な値の変化が必要 → だから損失関数を使う

**イメージ図：**
```
認識精度のグラフ（離散的）
   ┃ 34%  ━━━━━━━
   ┃ 33%  ━━━━━━━  ← 階段状（微分不可）
   ┃ 32%  ━━━━━━━
   ┗━━━━━━━━━━━━ 重み

損失関数のグラフ（連続的）
   ┃      ╱
   ┃    ╱          ← なめらか（微分可能）
   ┃  ╱
   ┗━━━━━━━━━━━━ 重み
```

---

## 4. 二乗和誤差（Mean Squared Error）

### 定義

```
E = (1/2) Σₖ (yₖ - tₖ)²
```

**各記号の意味：**
- `E`：誤差（Error）= 損失関数の値
- `yₖ`：ニューラルネットワークの出力（予測値）
- `tₖ`：正解ラベル（教師データ、Target）
- `k`：データの次元数（クラス数）
- `Σₖ`：すべてのkについて合計
- `(1/2)`：微分を簡単にするための係数（本質的には不要）

**直感的な理解：**
- 予測値と正解の差（yₖ - tₖ）を二乗して合計
- 差が大きいほど誤差が大きくなる
- 二乗することで、負の誤差も正の誤差も正の値になる

**なぜ二乗するのか？**
1. 負の値を正の値にする（距離の概念）
2. 大きな誤差にペナルティを与える（誤差10は誤差1の100倍のペナルティ）

### 実装

```python
import numpy as np

def mean_squared_error(y, t):
    return 0.5 * np.sum((y - t) ** 2)
```

**コードの説明：**
- `(y - t)`：予測値と正解の差を計算（配列の要素ごと）
- `** 2`：各要素を二乗
- `np.sum()`：すべての要素を合計
- `0.5 *`：1/2を掛ける

### 例

```python
# 正解ラベルは「2」（3番目の要素が1）
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]

# 例1：「2」の確率が最も高い（0.6 = 60%）
y1 = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
mean_squared_error(np.array(y1), np.array(t))  # 0.097...

# 例2：「7」の確率が最も高い（0.6 = 60%）
y2 = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]
mean_squared_error(np.array(y2), np.array(t))  # 0.597...
```

**結果の解釈：**
- y1の損失：0.097（小さい） → 正解に近い予測
- y2の損失：0.597（大きい） → 正解から遠い予測

**計算の詳細（y1の場合）：**
```
E = 0.5 × [(0.1-0)² + (0.05-0)² + (0.6-1)² + ... + (0.0-0)²]
  = 0.5 × [0.01 + 0.0025 + 0.16 + ... + 0]
  = 0.5 × 0.195
  = 0.0975
```

ポイント：`(0.6-1)² = 0.16` が最も大きな誤差項

---

## 5. 交差エントロピー誤差（Cross Entropy Error）

### 定義

```
E = -Σₖ tₖ log(yₖ)
```

**各記号の意味：**
- `E`：誤差（Error）= 損失関数の値
- `yₖ`：ニューラルネットワークの出力（予測確率）
- `tₖ`：正解ラベル（one-hot表現で、正解クラスだけ1、他は0）
- `log`：自然対数（底がe = 2.718...）
- `-`：負の符号（logは負の値なので、マイナスを付けて正の値に）

**直感的な理解：**

one-hot表現では、正解クラス以外の`tₖ = 0`なので、実質的に：
```
E = -log(y_正解クラス)
```

つまり、**正解クラスの予測確率だけに注目**します。

**log関数の特性：**
```
y = 1.0（100%確信） → log(1.0) = 0      → -log(1.0) = 0（誤差なし）
y = 0.5（50%確信）  → log(0.5) = -0.69  → -log(0.5) = 0.69
y = 0.1（10%確信）  → log(0.1) = -2.30  → -log(0.1) = 2.30（大きな誤差）
y = 0.01（1%確信）  → log(0.01) = -4.61 → -log(0.01) = 4.61（非常に大きな誤差）
```

**重要なポイント：**
- 正解クラスの確率が高い（1に近い） → 誤差小さい
- 正解クラスの確率が低い（0に近い） → 誤差大きい（急激に増加）

### one-hot表現

正解ラベルを1つだけ1で、他を0で表現する方法：

```python
# クラス「2」が正解の場合（10クラス分類）
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]
     ↑  ↑  ↑ 正解クラスだけ1
```

**なぜone-hotにするのか？**
- 数式と実装を統一できる
- すべてのクラスを同じ形式で扱える
- ニューラルネットワークの出力（確率分布）と形式が一致

### 実装

```python
import numpy as np

def cross_entropy_error(y, t):
    delta = 1e-7  # log(0)を防ぐための微小値
    return -np.sum(t * np.log(y + delta))
```

**コードの説明：**
- `delta = 1e-7`：log(0)は計算できない（-∞になる）ので、微小な値を足す
- `np.log(y + delta)`：各要素の自然対数を計算
- `t * np.log(...)`：正解クラス以外は t=0 なので、自動的に無視される
- `-np.sum(...)`：合計して負の符号を付ける

**なぜ delta が必要？**
```python
np.log(0)      # -inf（エラー）
np.log(0 + 1e-7)  # -16.11（計算可能）
```

### 例

```python
# 正解ラベルは「2」
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]

# 例1：「2」の確率が最も高い（0.6 = 60%）
y1 = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]
cross_entropy_error(np.array(y1), np.array(t))  # 0.510...

# 例2：「7」の確率が最も高い（0.6 = 60%）、「2」は0.1（10%）
y2 = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]
cross_entropy_error(np.array(y2), np.array(t))  # 2.302...
```

**結果の解釈：**
- y1の損失：0.510（小さい） → 正解クラスに60%の確信
- y2の損失：2.302（大きい） → 正解クラスに10%しか確信がない

**計算の詳細（y1の場合）：**
```
E = -(0×log(0.1) + 0×log(0.05) + 1×log(0.6) + ... + 0×log(0.0))
  = -log(0.6)  ← 正解クラス（t=1）の項だけが残る
  = -(-0.510)
  = 0.510
```

### なぜ交差エントロピーを使うのか？

**分類問題では交差エントロピー誤差の方が適している理由：**

1. **確率分布の差異を測るのに適している**
   - ニューラルネットワークの出力は確率分布（softmax）
   - 交差エントロピーは2つの確率分布の「距離」を測る指標

2. **学習が安定しやすい**
   - 誤差が大きいとき、勾配も大きくなる（学習が速い）
   - 誤差が小さいとき、勾配も小さくなる（細かく調整）

3. **ソフトマックス関数との相性が良い**
   - softmax + 交差エントロピーの組み合わせは微分が簡単
   - 逆伝播（第5章）で計算が簡潔になる

**比較：二乗和誤差 vs 交差エントロピー誤差**

| 特徴 | 二乗和誤差 | 交差エントロピー |
|------|-----------|----------------|
| **用途** | 回帰問題 | 分類問題 |
| **出力層** | 恒等関数 | ソフトマックス |
| **値の範囲** | 0 ~ ∞ | 0 ~ ∞ |
| **学習速度** | 普通 | 速い（分類問題で） |

---

## 6. ミニバッチ学習

### 問題：訓練データが大量

実際の機械学習では、データが大量にあります。

**MNISTの例：**
- 訓練データ：60,000枚の手書き数字画像
- 全データの損失関数を1回計算するのに時間がかかる
- 60,000回の予測 + 損失計算が必要

**全データを使う場合の計算量：**
```
1エポック（全データ1回） = 60,000回の予測
100エポック学習 = 6,000,000回の予測 → とても時間がかかる！
```

### 解決策：ミニバッチ学習

**一部のデータ（ミニバッチ）を使って損失関数を近似的に計算する方法**

**ミニバッチ学習の考え方：**
```
全データの平均損失 ≈ ランダムに選んだ一部データの平均損失
```

**なぜこれでうまくいくのか？**
- 大数の法則：十分な数のサンプルがあれば、サンプル平均は全体の平均に近づく
- 例：100個のバッチなら、全体の傾向をだいたい掴める

**計算量の比較：**
```
全データ（60,000個）：
  1回の更新 = 60,000個の損失計算 → 遅い

ミニバッチ（100個）：
  1回の更新 = 100個の損失計算 → 600倍速い！
```

### 実装（mini_batch.py）

```python
import sys, os
sys.path.append(os.pardir)
import numpy as np
from dataset.mnist import load_mnist

# MNISTデータの読み込み
(x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=False)

print(x_train.shape)  # (60000, 784) - 60000枚の画像
print(t_train.shape)  # (60000,) - 60000個のラベル

train_size = x_train.shape[0]  # 60000
batch_size = 10  # ミニバッチのサイズ

# ランダムに10個のインデックスを選択
batch_mask = np.random.choice(train_size, batch_size)
print(batch_mask)  # 例：[8159, 24835, 41976, 3384, 17238, 28567, 52143, 9812, 36457, 14892]

x_batch = x_train[batch_mask]  # 選ばれた10枚の画像データ
t_batch = t_train[batch_mask]  # 選ばれた10個のラベル
```

**`np.random.choice(60000, 10)`の動作：**
```
0〜59999の中から10個をランダムに選ぶ（重複なし）
例：
  実行1: [8159, 24835, 41976, ...]
  実行2: [1234, 45678, 9012, ...]  ← 毎回違う
```

**ポイント：**
- ランダムサンプリングで偏りを防ぐ
- 毎回異なるデータで学習（汎化性能向上）
- 計算量を大幅削減

### バッチ対応の交差エントロピー誤差（cross_entropy.py）

先ほどの `cross_entropy_error` 関数は1つのデータにしか対応していません。
複数のデータ（ミニバッチ）に対応するように拡張します。

```python
import numpy as np

def cross_entropy_error(y, t):
    # 1次元配列（単一データ）を2次元配列（バッチ）に変換
    if y.ndim == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)
        
    batch_size = y.shape[0]
    return -np.sum(t * np.log(y + 1e-7)) / batch_size
```

**データ形状の変換：**
```
単一データの場合：
  y.shape = (10,)        例：[0.1, 0.2, 0.6, ...]
  ↓ reshape
  y.shape = (1, 10)      例：[[0.1, 0.2, 0.6, ...]]

バッチデータの場合：
  y.shape = (100, 10)    例：[[0.1, 0.2, ...],  ← 1個目のデータ
                              [0.3, 0.1, ...],  ← 2個目のデータ
                              ...]
```

**バッチサイズで割る理由：**
- データ数に依らない比較可能な値にするため
- バッチサイズ10でも100でも、同じスケールで損失を評価できる

**計算の流れ：**
```
1. 各データの損失を計算
   データ1の損失：0.5
   データ2の損失：1.2
   ...
   データ100の損失：0.8

2. すべて合計：50.0

3. バッチサイズで割る：50.0 / 100 = 0.5（平均損失）
```

### one-hotではないラベルへの対応

実際には、ラベルが one-hot 表現ではなく、整数（0, 1, 2, ...）で与えられることもあります。

```python
# one-hot表現
t = [[0, 0, 1, 0, ...],   ← 正解クラス2
     [0, 1, 0, 0, ...],   ← 正解クラス1
     ...]

# 整数ラベル
t = [2, 1, 5, 0, ...]  ← よりコンパクト
```

**整数ラベルに対応したコード：**
```python
def cross_entropy_error(y, t):
    if y.ndim == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)
        
    batch_size = y.shape[0]
    
    # tがone-hotの場合（元の実装）
    # return -np.sum(t * np.log(y + 1e-7)) / batch_size
    
    # tが整数ラベル（0, 1, 2, ...）の場合
    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size
```

**`y[np.arange(batch_size), t]` の意味：**
```
例：batch_size = 3, t = [2, 1, 5]

np.arange(batch_size) = [0, 1, 2]  ← バッチ内のインデックス

y[np.arange(batch_size), t] = y[[0, 1, 2], [2, 1, 5]]
                             = [y[0, 2], y[1, 1], y[2, 5]]
                             ↑ 各データの正解クラスの予測確率を取得

具体例：
  y[0, 2]：1個目のデータのクラス2の予測確率
  y[1, 1]：2個目のデータのクラス1の予測確率
  y[2, 5]：3個目のデータのクラス5の予測確率
```

これで、one-hot表現と整数ラベルの両方に対応できます！

---

## 7. なぜ損失関数を設定するのか？

### 認識精度ではダメな理由

**認識精度は微分がほとんど0になる**

- 認識精度：正解か不正解かの離散的な値（33%、34%、35%...）
- わずかな重みの変化では、精度は変わらないことが多い
- 勾配がほとんど0 → 重みをどう更新すべきかわからない

### 損失関数の利点

**損失関数は連続的に変化する**

- 重みをわずかに変えれば、損失関数も連続的に変化
- 微分が意味を持つ
- 勾配（損失を減らす方向）がわかる

**ニューラルネットワークの学習では、連続的な値の変化が不可欠です。**

---

## 8. 数値微分（Numerical Differentiation）

### 微分とは

**微分**：ある瞬間の変化量（傾き）を表す

もっと具体的に言うと：
- ある点で、xを少し動かしたとき、関数の値がどれだけ変化するか
- グラフの接線の傾きと同じ

**微分の定義（極限を使った厳密な定義）：**
```
f'(x) = lim[h→0] (f(x+h) - f(x)) / h
```

**直感的な理解：**
```
     f(x+h) - f(x)    変化量
f'(x) = ─────────── = ───────
            h         移動量
```

**例：車の速度**
- 位置の変化 ÷ 時間 = 速度
- これが微分の考え方そのもの

### 数値微分の実装

実際のコンピュータでは「h → 0の極限」は計算できないので、
**十分小さなhを使って近似**します。

```python
def numerical_diff(f, x):
    h = 1e-4  # 0.0001（10のマイナス4乗）
    return (f(x + h) - f(x - h)) / (2 * h)
```

**なぜ h = 1e-4 なのか？**
- 小さすぎる（例：1e-50）：丸め誤差が大きくなる（コンピュータの計算精度の限界）
- 大きすぎる（例：1.0）：近似精度が悪くなる
- 1e-4 は良いバランス

**なぜ中心差分を使うのか？**

**前方差分（精度が低い）：**
```
f'(x) ≈ (f(x+h) - f(x)) / h

   f(x+h)
     ●
    ╱
   ╱  ← この傾きを使う（真の傾きとズレがある）
  ●
f(x)
```

**中心差分（精度が高い）：**
```
f'(x) ≈ (f(x+h) - f(x-h)) / (2h)

f(x+h) ●
        ╲
         ╲  ← この傾きを使う（真の傾きに近い）
        ╱
       ╱
f(x-h) ●
```

中心差分は、xの前後の情報を使うので、より正確な傾きを計算できます。

### 例：y = x² の微分

数学的には、`f(x) = x²` の微分は `f'(x) = 2x` です。

```python
def function_1(x):
    return x ** 2

# x=5での微分を数値的に計算
numerical_diff(function_1, 5)  
# 結果：9.999999999999787 ≈ 10

# 解析的な微分（理論値）
# f'(x) = 2x より、f'(5) = 2×5 = 10
```

**結果の解釈：**
- 数値微分：9.999999999999787
- 理論値：10.0
- 誤差：わずか 0.000000000000213（ほぼ一致！）

**数値微分は真の微分の近似値です。**

**視覚的なイメージ：**
```
y = x² のグラフ
  y
  |    
25|         ●  ← (5, 25)
  |       ╱
  |     ╱  傾き = 10
  |   ╱
  | ╱
  |╱__________ x
  0    5

x=5での接線の傾きが f'(5) = 10
```

**もう1つの例：x=10での微分**
```python
numerical_diff(function_1, 10)
# 結果：19.999999999999574 ≈ 20

# 理論値：f'(10) = 2×10 = 20
```

**まとめ：**
- 数値微分は計算機で微分を近似的に求める方法
- 実装は簡単だが、計算は遅い
- 精度はそこそこ良い（理論値にかなり近い）
- 後で学ぶ「誤差逆伝播法」の正しさを確認するのに使う

---

## 9. 偏微分（Partial Derivative）

### 偏微分とは

**多変数関数において、1つの変数についての微分**

例：`f(x₀, x₁) = x₀² + x₁²`

- `x₀`についての偏微分：`∂f/∂x₀ = 2x₀`
- `x₁`についての偏微分：`∂f/∂x₁ = 2x₁`

### 実装

```python
def function_2(x):
    return x[0]**2 + x[1]**2

# x₀=3, x₁=4の点での偏微分
def function_tmp1(x0):
    return x0**2 + 4.0**2

numerical_diff(function_tmp1, 3.0)  # 6.00000000... ≈ 6

def function_tmp2(x1):
    return 3.0**2 + x1**2

numerical_diff(function_tmp2, 4.0)  # 7.99999999... ≈ 8
```

---

## 10. 勾配（Gradient）

### 勾配とは

**勾配**：すべての変数の偏微分をまとめたベクトル

**数式：**
```
∇f = (∂f/∂x₀, ∂f/∂x₁, ..., ∂f/∂xₙ)
```

**もっと分かりやすく言うと：**
- 多変数関数において、「各変数をどう変えれば関数の値が変わるか」を示すベクトル
- 各要素は、その変数方向への傾き（偏微分）

**2変数の場合の例：**
```
関数：f(x₀, x₁) = x₀² + x₁²

勾配：∇f = (∂f/∂x₀, ∂f/∂x₁) = (2x₀, 2x₁)
```

### 実装

すべての変数について、1つずつ数値微分を計算していきます。

```python
def numerical_gradient(f, x):
    h = 1e-4  # 微小な値
    grad = np.zeros_like(x)  # xと同じ形状の配列（すべて0）
    
    for idx in range(x.size):
        tmp_val = x[idx]  # 元の値を保存
        
        # f(x+h)の計算
        x[idx] = tmp_val + h
        fxh1 = f(x)
        
        # f(x-h)の計算
        x[idx] = tmp_val - h
        fxh2 = f(x)
        
        # 中心差分で勾配を計算
        grad[idx] = (fxh1 - fxh2) / (2 * h)
        
        x[idx] = tmp_val  # 値を元に戻す（重要！）
        
    return grad
```

**アルゴリズムの流れ：**
```
1. x₀だけを少し動かして、f(x)の変化を見る → ∂f/∂x₀
2. x₁だけを少し動かして、f(x)の変化を見る → ∂f/∂x₁
3. x₂だけを少し動かして、f(x)の変化を見る → ∂f/∂x₂
...
→ すべての偏微分をまとめて勾配ベクトルを作る
```

**なぜ `x[idx] = tmp_val` が必要？**
```python
# 元の値を保存しないと...
x = [3.0, 4.0]
x[0] = x[0] + h  # x = [3.0001, 4.0]
x[0] = x[0] - h  # x = [3.0, 4.0] に戻らず...
x[1] = x[1] + h  # 次の計算がずれてしまう
```

### 勾配の意味

**勾配は「関数の値を最も大きく増やす方向」を示します。**

逆に言えば：
- **勾配の逆方向** = 関数の値を最も大きく減らす方向
- これが勾配降下法の基本原理

**具体例：f(x₀, x₁) = x₀² + x₁²**

この関数は、原点 (0, 0) で最小値0をとります。

```python
def function_2(x):
    return x[0]**2 + x[1]**2

# 点(3, 4)での勾配
numerical_gradient(function_2, np.array([3.0, 4.0]))
# 結果：array([6., 8.])
```

**結果の解釈：**
```
点(3, 4)での勾配 = [6.0, 8.0]

意味：
- x₀を1増やすと、f(x)は約6増える
- x₁を1増やすと、f(x)は約8増える

勾配の逆方向 = [-6.0, -8.0]
→ この方向に進めば、関数の値が最も減少する
→ つまり、原点(0, 0)に近づく
```

**もっと多くの点で勾配を計算：**
```python
# 点(0, 2)での勾配
numerical_gradient(function_2, np.array([0.0, 2.0]))
# 結果：array([0., 4.])
# → x₀方向の傾きは0、x₁方向は4

# 点(3, 0)での勾配
numerical_gradient(function_2, np.array([3.0, 0.0]))
# 結果：array([6., 0.])
# → x₀方向の傾きは6、x₁方向は0

# 点(0, 0)での勾配
numerical_gradient(function_2, np.array([0.0, 0.0]))
# 結果：array([0., 0.])
# → 最小値なので、どの方向にも傾きがない！
```

**勾配の可視化イメージ：**
```
f(x₀, x₁) = x₀² + x₁²  の等高線と勾配

 x₁
  ↑
  4│    ↙        勾配ベクトル（矢印）は
  3│  ↙    ↙    すべて原点を向いている
  2│↙    ●    ↙   （最小値の方向）
  1│  ↙    ↙
  0├──●────────→ x₀
  0  1  2  3  4

すべての勾配の逆方向が、原点(0,0)を指している
```

**重要なポイント：**
1. 勾配の方向 = 関数が最も増加する方向
2. 勾配の逆方向 = 関数が最も減少する方向
3. 勾配の大きさ = 変化の急峻さ（大きいほど急）
4. 勾配が0 = その点で関数が最小（または最大、または鞍点）

### 勾配法（Gradient Method）

勾配の情報を使って関数の最小値（または最大値）を探す手法：

- **勾配降下法（Gradient Descent）**：最小値を探す（機械学習で使う）
  - 勾配の**逆方向**に進む
  
- **勾配上昇法（Gradient Ascent）**：最大値を探す
  - 勾配の**順方向**に進む

**なぜ最小値を探すのか？**
- 機械学習では、損失関数（誤差）を最小化したい
- だから勾配降下法を使う

---

## 11. 勾配降下法（Gradient Descent）

### アルゴリズム

**更新式：**
```
x ← x - η∇f(x)
```

**各記号の意味：**
- `x`：更新する変数（ニューラルネットワークでは重みやバイアス）
- `η`（イータ、eta）：学習率（Learning Rate）
- `∇f(x)`：勾配ベクトル
- `←`：代入（xを新しい値に更新）

**直感的な理解：**
```
現在地 ← 現在地 - 学習率 × 勾配

勾配が正（上り坂） → マイナスすると下に移動
勾配が負（下り坂） → マイナスすると上に移動
→ 常に谷底（最小値）に向かって移動する
```

**学習率（Learning Rate）とは？**
- 1回の学習でどれだけ大きく移動するかを決めるパラメータ
- 大きすぎる → 発散（最小値を飛び越える）
- 小さすぎる → 学習が遅い（最小値に到達できない）
- 適切な値を選ぶのが重要（ハイパーパラメータ調整）

### 実装

```python
def gradient_descent(f, init_x, lr=0.01, step_num=100):
    """
    勾配降下法で関数fの最小値を探す
    
    Parameters:
    -----------
    f : function
        最適化したい関数
    init_x : numpy.ndarray
        初期値（スタート地点）
    lr : float
        学習率（Learning Rate）、デフォルト0.01
    step_num : int
        繰り返し回数、デフォルト100
        
    Returns:
    --------
    x : numpy.ndarray
        最適化後の変数（最小値の位置）
    """
    x = init_x  # 初期値をコピー
    
    for i in range(step_num):
        grad = numerical_gradient(f, x)  # 現在地での勾配を計算
        x -= lr * grad  # 勾配の逆方向に移動
        
    return x
```

**アルゴリズムの流れ：**
```
1. スタート地点（init_x）を設定
2. 現在地での勾配を計算
3. 勾配の逆方向に少し移動（学習率 × 勾配だけ）
4. step_num回繰り返す
5. 到達した場所を返す
```

### 例：最小値を探す

**問題：** f(x₀, x₁) = x₀² + x₁² の最小値を探す

理論的には、最小値は(0, 0)で値は0です。勾配降下法でこれを確認します。

```python
def function_2(x):
    return x[0]**2 + x[1]**2

# スタート地点：(-3.0, 4.0)
init_x = np.array([-3.0, 4.0])

# 勾配降下法を実行
result = gradient_descent(function_2, init_x, lr=0.1, step_num=100)
print(result)
# 結果：array([-6.11110793e-10,  8.14814391e-10])
#      ≈ [0, 0]  ← ほぼ原点に到達！
```

**結果の詳細：**
```
スタート：(-3.0, 4.0)
  ↓ 100回の更新
ゴール：(-0.00000000061, 0.00000000081)

関数値：
  f(-3.0, 4.0) = 9 + 16 = 25
  f(0, 0) = 0 + 0 = 0  ← 最小値に到達！
```

**学習の経過（イメージ）：**
```
ステップ1：(-3.0, 4.0) → (-2.4, 3.2)  勾配大、移動大
ステップ2：(-2.4, 3.2) → (-1.92, 2.56)
ステップ3：(-1.92, 2.56) → (-1.54, 2.05)
...
ステップ50：(-0.01, 0.01) → (-0.008, 0.008)  勾配小、移動小
...
ステップ100：ほぼ(0, 0)  最小値に到達
```

### 学習率の重要性

学習率は**ハイパーパラメータ**の中で最も重要なものの1つです。

#### ケース1：学習率が大きすぎる

```python
result = gradient_descent(function_2, init_x, lr=10.0, step_num=100)
print(result)
# 結果：array([-2.58983747e+13, -1.29524862e+12])
#       発散！値が巨大になってしまう
```

**何が起こったか？**
```
更新幅が大きすぎて、最小値を飛び越えてしまう

  最小値 ●
      ╱   ╲
    ╱       ╲
  ●  →→→→  ●  飛び越えてしまう
スタート    着地（さらに遠く）

次の更新でもっと遠くに...（発散）
```

#### ケース2：学習率が小さすぎる

```python
result = gradient_descent(function_2, init_x, lr=1e-10, step_num=100)
print(result)
# 結果：array([-2.99999994, 3.99999992])
#       ほとんど動いていない！
```

**何が起こったか？**
```
更新幅が小さすぎて、ほとんど移動できない

スタート●→ ←わずかしか動かない
  (-3, 4)

100回更新しても、ほぼ同じ場所
```

#### ケース3：適切な学習率

```python
result = gradient_descent(function_2, init_x, lr=0.1, step_num=100)
# 結果：ほぼ(0, 0)  ← 成功！
```

**学習率選択のポイント：**
```
┌──────────────────────────────────────┐
│ 学習率 │ 結果                        │
├────────┼─────────────────────────────┤
│ 大     │ 発散（値が爆発的に大きくなる）│
│ 適切   │ 最小値に到達                │
│ 小     │ 学習が遅い、最小値に届かない  │
└──────────────────────────────────────┘
```

**適切な学習率の見つけ方：**
1. まず大きめの値（例：0.1）から試す
2. 発散したら小さくする（例：0.01）
3. 学習が遅ければ大きくする（例：1.0）
4. 損失関数のグラフを見ながら調整

**実践的なヒント：**
- 一般的な初期値：0.01, 0.001, 0.0001
- AdamやRMSpropなどの最適化手法を使うと、自動調整される（第6章）
- 学習曲線を見て、滑らかに減少しているか確認

### まとめ：勾配降下法のポイント

**メリット：**
- シンプルで理解しやすい
- 多変数関数に対応できる
- ニューラルネットワークの学習に応用できる

**デメリット：**
- 学習率の調整が難しい
- 局所最小値に陥ることがある
- 計算量が多い（変数が多いと遅い）

**改良版（第6章で学ぶ）：**
- SGD（Stochastic Gradient Descent）：ミニバッチを使う
- Momentum：慣性を利用
- AdaGrad, Adam：学習率を自動調整

---

## 12. ニューラルネットワークに対する勾配

### 重みに対する損失関数の勾配

これまで学んだ勾配降下法を、ニューラルネットワークの学習に応用します。

**ニューラルネットワークの学習目標：**
```
損失関数 L に対する重み W の勾配 ∂L/∂W を計算する
```

**この勾配が意味すること：**
- 「各重みをどう変えれば、損失（誤差）が減るか」を教えてくれる
- 勾配の逆方向に重みを更新すれば、性能が向上する

**具体例で考えると：**
```
重みW[0,1]の勾配が+2.5の場合：
  → W[0,1]を増やすと損失も増える
  → W[0,1]を減らせば損失が減る
  → だから W[0,1] -= 学習率 × 2.5

重みW[1,2]の勾配が-1.3の場合：
  → W[1,2]を増やすと損失が減る
  → だから W[1,2] -= 学習率 × (-1.3) = W[1,2] += 学習率 × 1.3
```

### 簡単なニューラルネットワークの例

教材用の小さなネットワークで、勾配計算を実験してみます。

```python
import sys, os
sys.path.append(os.pardir)
import numpy as np
from common.functions import softmax, cross_entropy_error
from common.gradient import numerical_gradient

class simpleNet:
    def __init__(self):
        # 2×3の重み行列（ランダム初期化）
        self.W = np.random.randn(2, 3)
        # 例：array([[ 0.47,  0.89, -1.15],
        #           [-0.33,  1.09,  0.21]])
        
    def predict(self, x):
        """予測（順伝播）"""
        return np.dot(x, self.W)
        
    def loss(self, x, t):
        """損失関数の計算"""
        z = self.predict(x)        # 重み付き和
        y = softmax(z)             # ソフトマックス関数
        loss = cross_entropy_error(y, t)  # 交差エントロピー誤差
        return loss
```

**ネットワークの構造：**
```
入力層（2次元）  重み（2×3）  出力層（3次元）
   x[0] ─┐
         ├─→ W ─→ [ y[0], y[1], y[2] ]
   x[1] ─┘         (softmax)
```

### 勾配の計算

```python
# ネットワークのインスタンス作成
net = simpleNet()
print(net.W)
# [[ 0.47355232  0.89364953 -1.15584521]
#  [-0.33508465  1.09555361  0.21064111]]

# 入力データ
x = np.array([0.6, 0.9])

# 正解ラベル（one-hot、クラス2が正解）
t = np.array([0, 0, 1])

# 予測
p = net.predict(x)
print(p)
# [ 0.18280087  1.52286588 -0.5045026 ]
# → クラス1の値が最も大きい（でも正解はクラス2）

# ソフトマックス後の確率
y = softmax(p)
print(y)
# [ 0.21924763  0.78718784 -0.11281009]
# → クラス1の確率78%（誤った予測）

# 損失
loss = net.loss(x, t)
print(loss)
# 2.3025...（損失が大きい = 予測が悪い）
```

**勾配の計算：**
```python
# lambda式で、Wを変数とする損失関数を定義
def f(W):
    return net.loss(x, t)

# 重みWに対する勾配を数値的に計算
dW = numerical_gradient(f, net.W)
print(dW)
# [[ 0.21924763  0.14356247 -0.36281009]
#  [ 0.32887144  0.2153437  -0.54421514]]
```

**結果の形状：**
```
重みWの形状：(2, 3)
勾配dWの形状：(2, 3)  ← 重みと同じ形状

各要素が、その重みに対する損失の傾きを表す
```

### 勾配の解釈

勾配の各要素を詳しく見てみましょう。

```python
print("重みW：")
print(net.W)
# [[ 0.47  0.89 -1.16]   ← 1行目：x[0]から出力への重み
#  [-0.34  1.10  0.21]]  ← 2行目：x[1]から出力への重み

print("勾配dW：")
print(dW)
# [[ 0.22  0.14 -0.36]   ← 1行目：x[0]側の重みの勾配
#  [ 0.33  0.22 -0.54]]  ← 2行目：x[1]側の重みの勾配
```

**特に注目すべき要素：**

**dW[0, 2] = -0.36281009（負の値）**
```
意味：W[0, 2]を増やせば損失が減る
理由：正解クラスは2番目、その重みを大きくすれば正解の出力が大きくなる
更新：W[0, 2] -= 学習率 × (-0.36) → W[0, 2]が増加
```

**dW[1, 0] = 0.32887144（正の値）**
```
意味：W[1, 0]を減らせば損失が減る
理由：クラス0は不正解、その重みを小さくすれば不正解の出力が小さくなる
更新：W[1, 0] -= 学習率 × 0.33 → W[1, 0]が減少
```

**パターン：**
```
正解クラス（クラス2）への重み：勾配が負
  → 重みを増やす方向に更新（正解の出力を強化）

不正解クラス（クラス0, 1）への重み：勾配が正
  → 重みを減らす方向に更新（不正解の出力を抑制）
```

### 重みの更新

勾配を使って、実際に重みを更新してみます。

```python
# 学習率
lr = 0.1

# 勾配降下法で重みを更新
net.W -= lr * dW

print("更新後の重み：")
print(net.W)
# [[ 0.45  0.88 -1.12]   ← クラス2への重みが増加
#  [-0.37  1.07  0.27]]  ← クラス2への重みが増加

# 更新後の損失を確認
new_loss = net.loss(x, t)
print(f"更新前の損失：{loss:.4f}")
print(f"更新後の損失：{new_loss:.4f}")
# 更新前の損失：2.3025
# 更新後の損失：2.1234  ← 損失が減少！
```

**重要なポイント：**
1. 勾配は「重みをどう調整すべきか」を教えてくれる
2. 勾配の逆方向に重みを更新すれば、損失が減る
3. これを繰り返すことで、ネットワークが学習する

### 勾配とニューラルネットワークの学習

**全体の流れ：**
```
1. 順伝播（Forward）
   入力x → 重みW → 出力y → 損失L

2. 勾配計算（Gradient）
   ∂L/∂W を計算（各重みに対する損失の傾き）

3. 重み更新（Update）
   W ← W - 学習率 × ∂L/∂W

4. 1〜3を繰り返す
   → 損失が徐々に減少
   → ネットワークの性能が向上
```

**数値微分の限界：**
- この例では数値微分を使ったが、実用的には遅すぎる
- 重みが多い（数万〜数百万個）と計算時間が膨大
- 次章で学ぶ**誤差逆伝播法**は、勾配を高速に計算できる

**数値微分の役割：**
- 理論を理解するため（今回）
- 誤差逆伝播法の実装が正しいか確認するため（勾配チェック）

---

## 13. 学習アルゴリズムの実装

### 前提

ニューラルネットワークには、適応可能な重みとバイアスがあり、訓練データに適応するように調整することを**学習**と呼びます。

### ニューラルネットワークの学習手順

**ステップ1：ミニバッチ**
訓練データからランダムに一部のデータを選ぶ

**ステップ2：勾配の計算**
各重みパラメータに対する損失関数の勾配を求める

**ステップ3：パラメータの更新**
勾配方向に重みパラメータを更新

**ステップ4：繰り返し**
ステップ1〜3を繰り返す

**これが「確率的勾配降下法（SGD: Stochastic Gradient Descent）」です。**

### 2層ニューラルネットワークのクラス

```python
class TwoLayerNet:
    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):
        # 重みの初期化
        self.params = {}
        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
        self.params['b1'] = np.zeros(hidden_size)
        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)
        self.params['b2'] = np.zeros(output_size)
        
    def predict(self, x):
        W1, W2 = self.params['W1'], self.params['W2']
        b1, b2 = self.params['b1'], self.params['b2']
        
        a1 = np.dot(x, W1) + b1
        z1 = sigmoid(a1)
        a2 = np.dot(z1, W2) + b2
        y = softmax(a2)
        
        return y
        
    def loss(self, x, t):
        y = self.predict(x)
        return cross_entropy_error(y, t)
        
    def accuracy(self, x, t):
        y = self.predict(x)
        y = np.argmax(y, axis=1)
        t = np.argmax(t, axis=1)
        
        accuracy = np.sum(y == t) / float(x.shape[0])
        return accuracy
        
    def numerical_gradient(self, x, t):
        loss_W = lambda W: self.loss(x, t)
        
        grads = {}
        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])
        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])
        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])
        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])
        
        return grads
```

### 学習の実装

```python
# データの読み込み
(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)

network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

# ハイパーパラメータ
iters_num = 10000
train_size = x_train.shape[0]
batch_size = 100
learning_rate = 0.1

train_loss_list = []
train_acc_list = []
test_acc_list = []

# 1エポックあたりの繰り返し数
iter_per_epoch = max(train_size / batch_size, 1)

for i in range(iters_num):
    # ミニバッチの取得
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]
    
    # 勾配の計算
    grad = network.numerical_gradient(x_batch, t_batch)
    
    # パラメータの更新
    for key in ('W1', 'b1', 'W2', 'b2'):
        network.params[key] -= learning_rate * grad[key]
    
    # 学習経過の記録
    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)
    
    # 1エポックごとに精度を計算
    if i % iter_per_epoch == 0:
        train_acc = network.accuracy(x_train, t_train)
        test_acc = network.accuracy(x_test, t_test)
        train_acc_list.append(train_acc)
        test_acc_list.append(test_acc)
        print(f"train acc, test acc | {train_acc}, {test_acc}")
```

**エポック（Epoch）**：すべての訓練データを1回学習し終えた単位

---

## データサイエンスにおける重要ポイント

### 1. 損失関数の選択

| 問題 | 損失関数 |
|------|---------|
| **回帰** | 二乗和誤差（MSE） |
| **分類** | 交差エントロピー誤差 |

### 2. ハイパーパラメータ

**ハイパーパラメータ**：学習前に設定するパラメータ

- **学習率（Learning Rate）**：更新の大きさ
- **バッチサイズ**：一度に処理するデータ数
- **エポック数**：訓練データを何回繰り返すか
- **隠れ層のニューロン数**

**これらは試行錯誤で調整する必要があります。**

### 3. 過学習の防止

- **訓練データとテストデータを分ける**
- **検証データ（Validation Data）**でハイパーパラメータを調整
- 正則化（Regularization）を使用（第6章）
- Dropout（第6章）

### 4. 学習の可視化

**学習曲線（Learning Curve）**を描くことで、学習の進行状況を把握：

```python
import matplotlib.pyplot as plt

plt.plot(train_loss_list)
plt.xlabel("Iteration")
plt.ylabel("Loss")
plt.show()
```

### 5. ミニバッチ学習の利点

- **計算速度の向上**：全データより高速
- **メモリ効率**：大規模データセットでも扱える
- **汎化性能の向上**：ノイズによる正則化効果

### 6. 数値微分 vs 誤差逆伝播法

**数値微分**：
- 実装が簡単
- 計算が遅い
- 勾配の確認に使用

**誤差逆伝播法（第5章）**：
- 計算が高速
- 実装が複雑
- 実際の学習で使用

---

## まとめ

**第4章で学んだこと：**

1. **学習とは**
   - データから最適な重みパラメータを見つけること
   - 特徴量も自動で学習できる

2. **訓練データとテストデータ**
   - 汎化能力の評価
   - 過学習の検出

3. **損失関数**
   - 二乗和誤差（回帰問題）
   - 交差エントロピー誤差（分類問題）
   - なぜ認識精度ではダメか

4. **ミニバッチ学習**
   - 一部のデータで効率的に学習
   - ランダムサンプリング

5. **数値微分と勾配**
   - 微分：瞬間の変化量
   - 偏微分：多変数関数の微分
   - 勾配：すべての変数の偏微分

6. **勾配降下法**
   - 勾配に従ってパラメータを更新
   - 学習率の重要性

7. **学習アルゴリズムの実装**
   - ミニバッチ → 勾配計算 → 更新 → 繰り返し
   - 確率的勾配降下法（SGD）

**次章への準備：**

第5章では、**誤差逆伝播法（Backpropagation）**を学びます。数値微分より遥かに高速に勾配を計算できる手法で、実用的なディープラーニングに不可欠です。

**キーワード：**
- 計算グラフ
- 連鎖律（Chain Rule）
- 逆伝播
- 高速な勾配計算

---

## 補足：よく使う用語

### 学習関連

- **訓練（Training）**：モデルを学習させること
- **推論（Inference）**：学習済みモデルで予測すること
- **エポック（Epoch）**：全訓練データを1回学習した単位
- **イテレーション（Iteration）**：パラメータ更新1回

### パラメータ

- **重み（Weight）**：学習で調整されるパラメータ
- **バイアス（Bias）**：学習で調整されるパラメータ
- **ハイパーパラメータ**：学習前に設定するパラメータ

### 最適化

- **勾配降下法（Gradient Descent）**：基本的な最適化手法
- **SGD（Stochastic Gradient Descent）**：確率的勾配降下法
- **学習率（Learning Rate）**：更新の大きさを決めるパラメータ

### 評価

- **損失（Loss）**：モデルの性能の悪さを示す指標
- **精度（Accuracy）**：正解率
- **汎化（Generalization）**：未知データへの対応能力
