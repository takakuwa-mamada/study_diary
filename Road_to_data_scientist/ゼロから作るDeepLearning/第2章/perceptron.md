# パーセプトロン（Perceptron）

## 第2章の概要

この章では、ニューラルネットワークの起源である**パーセプトロン**について学びます。パーセプトロンは非常にシンプルなアルゴリズムですが、深層学習の基礎となる重要な概念を含んでいます。

---

## パーセプトロンとは

パーセプトロンは、**ニューラルネットワーク（深層学習）の起源**となるアルゴリズムです。1957年にFrank Rosenblattによって考案されました。

**歴史的背景：**
- 1943年：McCulloch-Pittsニューロンモデル提案
- 1957年：Rosenblattがパーセプトロンを考案
- 1969年：Minsky & Papertが単層パーセプトロンの限界を指摘
- 1986年：誤差逆伝播法の発見により、多層ニューラルネットワークが実用化

### 基本的な仕組み

パーセプトロンは**人間の脳の神経細胞（ニューロン）を模倣した数学モデル**です。

- パーセプトロンは**複数の信号を受け取り、一つの信号を出力する**
- 入力信号には0または1の2値をとる
- 出力も0または1の2値

**生物学的なニューロンとの対応：**
```
生物学的ニューロン       パーセプトロン
─────────────────────────────────────
樹状突起（入力）    →   入力信号 (x₁, x₂, ...)
シナプス（結合強度） →   重み (w₁, w₂, ...)
細胞体（処理）      →   重み付き和 + 閾値判定
軸索（出力）        →   出力信号 (y)
```

**視覚的なイメージ：**
```
入力    重み      ニューロン
x₁ ───→ w₁ ──┐
              ├─→ Σ → [閾値判定] → y（出力）
x₂ ───→ w₂ ──┘
```

## 2入力パーセプトロンの数式

2つの入力信号を受け取るパーセプトロンは、以下の数式で表されます：

```
y = { 0  (w₁x₁ + w₂x₂ ≤ θ)
    { 1  (w₁x₁ + w₂x₂ > θ)
```

**記号の説明：**
- `x₁, x₂`：入力信号（0 または 1）
- `w₁, w₂`：**重み（weight）** - 各入力信号の重要度を調整するパラメータ
- `θ`（シータ、theta）：**閾値（threshold）** - 出力を決定する境界値
- `y`：出力信号（0 または 1）

**計算の流れ：**
```
ステップ1：各入力に重みを掛ける
  x₁ × w₁ + x₂ × w₂

ステップ2：合計値を閾値と比較
  合計 ≤ θ なら y = 0（発火しない）
  合計 > θ なら y = 1（発火する）
```

**具体例：**
```python
# 例：w₁=0.5, w₂=0.5, θ=0.7
x₁=1, x₂=1 の場合：
  0.5×1 + 0.5×1 = 1.0
  1.0 > 0.7 → y = 1（発火！）

x₁=1, x₂=0 の場合：
  0.5×1 + 0.5×0 = 0.5
  0.5 ≤ 0.7 → y = 0（発火しない）
```

### 重みの意味

**重み（weight）は各入力信号の重要度を制御します。**

もっと詳しく言うと：
- 重みが大きい → その入力信号が結果に与える影響が大きい
- 重みが小さい → その入力信号の影響は小さい
- 重みが負 → その入力信号は出力を抑制する（逆効果）
- 重みを調整することで、パーセプトロンの振る舞いを変えられる

**例え話：投票システム**
```
選挙で2人の推薦を受けて候補者を決める場合：

重みw₁=0.8（部長の推薦）  ← 重要度高い
重みw₂=0.2（平社員の推薦）← 重要度低い

部長が推薦（x₁=1）すれば、0.8点
平社員が推薦（x₂=1）すれば、0.2点

合計が閾値θ=0.9を超えれば採用（y=1）

→ 部長だけの推薦では不十分（0.8 < 0.9）
→ 両方の推薦があれば採用（1.0 > 0.9）
```

## バイアス（bias）を導入した表現

上記の式を、**バイアス b = -θ** として書き換えると：

```
y = { 0  (b + w₁x₁ + w₂x₂ ≤ 0)
    { 1  (b + w₁x₁ + w₂x₂ > 0)
```

**なぜバイアスを導入するのか？**
1. 数式がシンプルになる（閾値θを左辺に移動してb=-θとする）
2. ニューラルネットワークの実装で扱いやすい
3. 機械学習の標準的な表現方法

**バイアスの役割：**
- バイアスは「発火のしやすさ」を調整するパラメータ
- バイアスが大きい（正の値）→ ニューロンが発火しやすくなる
- バイアスが小さい（負の値）→ ニューロンが発火しにくくなる

**具体例で理解：**
```
ケース1：b = 0.5（大きい正の値）
  0.5 + 0.3×x₁ + 0.3×x₂ > 0
  → x₁=0, x₂=0 でも 0.5 > 0 → 発火しやすい！

ケース2：b = -0.9（大きい負の値）
  -0.9 + 0.3×x₁ + 0.3×x₂ > 0
  → x₁=1, x₂=1 でも -0.9+0.6=-0.3 < 0 → 発火しにくい

ケース3：b = 0
  0 + 0.3×x₁ + 0.3×x₂ > 0
  → 入力次第（中立）
```

**重みとバイアスの違い：**
```
┌──────────────────────────────────┐
│ パラメータ │ 役割              │
├───────────┼───────────────────┤
│ 重み (w)  │ 入力の重要度を制御 │
│ バイアス (b)│ 発火しやすさを制御│
└──────────────────────────────────┘

比喩：温度調節
  重み → どの部屋（入力）からの熱を重視するか
  バイアス → 全体の温度のベースライン
```

## 論理回路の実装

パーセプトロンを使って、基本的な論理回路を実装できます：

### ANDゲート
両方の入力が1のときだけ出力が1

**パラメータ例：** `w₁=0.5, w₂=0.5, θ=0.7` または `w₁=0.5, w₂=0.5, b=-0.7`

### NANDゲート（NOT AND）
ANDゲートの出力を反転

**パラメータ例：** `w₁=-0.5, w₂=-0.5, b=0.7`

### ORゲート
いずれかの入力が1のとき出力が1

**パラメータ例：** `w₁=0.5, w₂=0.5, b=-0.2`

## パーセプトロンの限界

**単層パーセプトロンでは線形分離可能な問題しか解けません。**

**線形分離可能とは？**
- データを1本の直線（2次元の場合）で2つのグループに分けられること
- 3次元なら1枚の平面、高次元なら1つの超平面で分離可能

### XORゲートの問題

XOR（排他的論理和）は、単層パーセプトロンでは実装できません：

**真理値表：**
| x₁ | x₂ | y（XOR）|
|----|----|---------|
| 0  | 0  | 0       |
| 1  | 0  | 1       |
| 0  | 1  | 1       |
| 1  | 1  | 0       |

**なぜ単層パーセプトロンで実装できないのか？**

XORは**非線形分離問題**であり、1本の直線では分離できません。

**視覚的な理解：**
```
2次元平面にプロット：

y=1の点: (1,0), (0,1)  ← ●で表示
y=0の点: (0,0), (1,1)  ← ○で表示

  x₂
   ↑
 1 │ ●     ○
   │
 0 │ ○     ●
   └─────────→ x₁
     0     1

どんな直線を引いても、●と○を完全に分離できない！
```

**線形分離可能な例（AND, OR）：**
```
ANDゲート：
  x₂
   ↑
 1 │ ○     ●   ← 直線で分離可能
   │  ＼
 0 │ ○  ＼  ○
   └─────────→ x₁
     0     1

ORゲート：
  x₂
   ↑
 1 │ ●     ●
   │     ／
 0 │ ○ ／  ●   ← 直線で分離可能
   └─────────→ x₁
     0     1
```

**数学的な証明：**

単層パーセプトロンの式：
```
y = { 1  (w₁x₁ + w₂x₂ + b > 0)
    { 0  (otherwise)
```

XORを満たす w₁, w₂, b は存在しない：
```
必要な条件：
  0×w₁ + 0×w₂ + b ≤ 0  →  b ≤ 0     ... (1)
  1×w₁ + 0×w₂ + b > 0  →  w₁ + b > 0 ... (2)
  0×w₁ + 1×w₂ + b > 0  →  w₂ + b > 0 ... (3)
  1×w₁ + 1×w₂ + b ≤ 0  →  w₁ + w₂ + b ≤ 0 ... (4)

(2)と(3)より: w₁ > -b, w₂ > -b
(1)より: b ≤ 0 なので -b ≥ 0
よって: w₁ > 0, w₂ > 0

しかし(4)より: w₁ + w₂ ≤ -b
(1)より: -b ≥ 0
つまり: w₁ + w₂ ≤ 0

矛盾！（w₁ > 0 かつ w₂ > 0 なら w₁ + w₂ > 0 のはず）
```

この矛盾により、XORを表現する単層パーセプトロンは存在しません。

## 多層パーセプトロン

**解決策：パーセプトロンを多層に重ねる**

XORは、既存のゲート（AND, NAND, OR）を組み合わせることで実装できます。

**XORの構成：**
```
XOR = (x₁ NAND x₂) AND (x₁ OR x₂)
```

**詳細な計算：**
```
ステップ1：中間層を計算
  s₁ = NAND(x₁, x₂)  ← 第1層
  s₂ = OR(x₁, x₂)    ← 第1層

ステップ2：出力層を計算
  y = AND(s₁, s₂)    ← 第2層
```

**真理値表で確認：**
| x₁ | x₂ | s₁（NAND）| s₂（OR）| y（AND）| 正解（XOR）|
|----|----|-----------|---------|---------|-----------| 
| 0  | 0  | 1         | 0       | 0       | 0 ✓       |
| 1  | 0  | 1         | 1       | 1       | 1 ✓       |
| 0  | 1  | 1         | 1       | 1       | 1 ✓       |
| 1  | 1  | 0         | 1       | 0       | 0 ✓       |

すべて一致！

**ネットワーク構造の図：**
```
入力層    第1層（中間層）  第2層（出力層）
           NAND
x₁ ─────→   s₁   ─────┐
    ╲                  ├─→ AND → y（XOR）
     ╲    OR           ╱
      ╲→  s₂   ───────┘
x₂ ─────┘

層の数え方：
  0層目（入力層）：x₁, x₂
  1層目（中間層）：s₁, s₂
  2層目（出力層）：y

→ これは「2層パーセプトロン」と呼ばれる
```

このように、**2層のパーセプトロンでXORを実装可能**です。

**重要な洞察：**
- 単層では線形分離可能な問題のみ解決可能
- 多層にすることで、非線形問題も解決可能
- 層を増やすほど、より複雑なパターンを表現できる
- これが「深層学習（Deep Learning）」の基本原理

## データサイエンスにおける重要なポイント

### 1. 特徴量の重要度（重み）
データサイエンスでは、どの特徴量が予測に重要かを理解することが重要です。パーセプトロンの重みは、この**特徴量の重要度**を表現しています。

### 2. 線形分離可能性
多くの機械学習問題において、データが線形分離可能かどうかは重要な判断基準です。
- **線形分離可能**：単純なモデルで高精度が得られる
- **非線形分離**：より複雑なモデル（多層ニューラルネットワークなど）が必要

### 3. 層を重ねることの重要性
単層では表現できない複雑なパターンも、**層を重ねることで表現可能**になります。これが深層学習（ディープラーニング）の基本原理です。

### 4. パラメータの学習
実際の機械学習では、重みとバイアスを**データから自動的に学習**します（パーセプトロンの例では手動で設定しましたが）。この学習プロセスが「訓練（training）」です。

## まとめ

- パーセプトロンは複数の入力から一つの出力を生成する基本的なニューロンモデル
- 重みは各入力の重要度、バイアスは発火のしやすさを制御
- 単層パーセプトロンは線形分離可能な問題のみ解決可能
- 多層化することで、より複雑な非線形問題も解決可能
- これがニューラルネットワークと深層学習の基礎となる