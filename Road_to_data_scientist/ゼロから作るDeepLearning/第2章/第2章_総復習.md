# 第2章 総復習：パーセプトロン

## 📚 この章の学習目標

**ニューラルネットワークの起源となるアルゴリズムを理解する**

```
パーセプトロン
    ↓
重みとバイアス
    ↓
論理回路の実装
    ↓
単層の限界（XOR問題）
    ↓
多層化の必要性
    ↓
ニューラルネットワークへ
```

---

## 1. パーセプトロンの基本

### 1.1 パーセプトロンとは

**複数の信号を受け取り、一つの信号を出力するアルゴリズム**

```
入力    重み      ニューロン
x₁ ───→ w₁ ──┐
              ├─→ Σ → [閾値判定] → y（出力）
x₂ ───→ w₂ ──┘
```

**流れ：**
1. 入力信号 $x_1, x_2$ を受け取る
2. 各入力に重み $w_1, w_2$ を掛ける
3. 加重和を計算
4. 閾値と比較して出力を決定

---

### 1.2 数式表現

#### 基本式

$$
y = \begin{cases}
0 & (w_1x_1 + w_2x_2 \leq \theta) \\
1 & (w_1x_1 + w_2x_2 > \theta)
\end{cases}
$$

**記号の意味：**
- $x_1, x_2$：**入力信号**（0 または 1）
- $w_1, w_2$：**重み** - 入力の重要度
- $\theta$：**閾値** - 発火の境界値
- $y$：**出力信号**（0 または 1）

#### バイアス表記

$$
y = \begin{cases}
0 & (b + w_1x_1 + w_2x_2 \leq 0) \\
1 & (b + w_1x_1 + w_2x_2 > 0)
\end{cases}
$$

ここで $b = -\theta$（**バイアス**）

**バイアス導入のメリット：**
- 式が簡潔になる
- 実装しやすい
- ニューラルネットワークとの統一性

---

### 1.3 重みとバイアスの役割

```
┌─────────────────────────────────┐
│ パラメータ │ 役割              │
├───────────┼───────────────────┤
│ 重み (w)  │ 入力の重要度を制御 │
│           │ 大きい→重要       │
│           │ 小さい→軽視       │
├───────────┼───────────────────┤
│ バイアス(b)│ 発火しやすさを制御│
│           │ 大きい→発火しやすい│
│           │ 小さい→発火しにくい│
└─────────────────────────────────┘
```

**具体例：**
```python
# 重みが大きい → x1の影響が強い
w1 = 0.8  # x1の重み（大）
w2 = 0.2  # x2の重み（小）

# バイアスが正 → 発火しやすい
b = 0.5   # 正のバイアス
```

---

## 2. 論理回路の実装

### 2.1 ANDゲート

**真理値表：**
| x₁ | x₂ | y |
|----|----|---|
| 0  | 0  | 0 |
| 1  | 0  | 0 |
| 0  | 1  | 0 |
| 1  | 1  | 1 |

**実装：**
```python
def AND(x1, x2):
    w1, w2, b = 0.5, 0.5, -0.7
    tmp = x1*w1 + x2*w2 + b
    if tmp <= 0:
        return 0
    else:
        return 1

# テスト
print(AND(0, 0))  # 0
print(AND(1, 0))  # 0
print(AND(0, 1))  # 0
print(AND(1, 1))  # 1
```

**パラメータの意味：**
```
w1 = 0.5, w2 = 0.5, b = -0.7

計算例：
AND(0, 0): 0×0.5 + 0×0.5 + (-0.7) = -0.7 ≤ 0 → 0
AND(1, 0): 1×0.5 + 0×0.5 + (-0.7) = -0.2 ≤ 0 → 0
AND(0, 1): 0×0.5 + 1×0.5 + (-0.7) = -0.2 ≤ 0 → 0
AND(1, 1): 1×0.5 + 1×0.5 + (-0.7) =  0.3 > 0 → 1
```

**視覚的理解：**
```
  x₂
   ↑
 1 │ ○     ●   ← (1,1)のみ1
   │  ＼
 0 │ ○  ＼  ○
   └─────────→ x₁
     0     1
   
直線 w₁x₁ + w₂x₂ + b = 0 で分離
```

---

### 2.2 NANDゲート

**真理値表：**
| x₁ | x₂ | y |
|----|----|---|
| 0  | 0  | 1 |
| 1  | 0  | 1 |
| 0  | 1  | 1 |
| 1  | 1  | 0 |

**実装：**
```python
def NAND(x1, x2):
    w1, w2, b = -0.5, -0.5, 0.7
    tmp = x1*w1 + x2*w2 + b
    if tmp <= 0:
        return 0
    else:
        return 1
```

**ANDとの関係：**
```
NAND = NOT AND（ANDの否定）

パラメータの関係：
AND:  w1=0.5,  w2=0.5,  b=-0.7
NAND: w1=-0.5, w2=-0.5, b=0.7
      ↑ 符号を反転
```

---

### 2.3 ORゲート

**真理値表：**
| x₁ | x₂ | y |
|----|----|---|
| 0  | 0  | 0 |
| 1  | 0  | 1 |
| 0  | 1  | 1 |
| 1  | 1  | 1 |

**実装：**
```python
def OR(x1, x2):
    w1, w2, b = 0.5, 0.5, -0.2
    tmp = x1*w1 + x2*w2 + b
    if tmp <= 0:
        return 0
    else:
        return 1
```

**ANDとの違い：**
```
AND: b = -0.7（厳しい閾値）
OR:  b = -0.2（緩い閾値）

→ ORの方が発火しやすい
```

---

## 3. 単層パーセプトロンの限界

### 3.1 線形分離可能性

**単層パーセプトロンは1本の直線で分離できる問題のみ解決可能**

```
線形分離可能な例（AND, OR, NAND）：

ANDゲート：
  x₂
   ↑
 1 │ ○     ●
   │  ＼
 0 │ ○  ＼  ○   ← 直線で分離可能
   └─────────→ x₁
     0     1

ORゲート：
  x₂
   ↑
 1 │ ●     ●
   │    ＼
 0 │ ○    ＼ ●   ← 直線で分離可能
   └─────────→ x₁
     0     1
```

---

### 3.2 XORゲートの問題 ⭐⭐⭐

#### 真理値表

| x₁ | x₂ | y（XOR）|
|----|----|---------|
| 0  | 0  | 0       |
| 1  | 0  | 1       |
| 0  | 1  | 1       |
| 1  | 1  | 0       |

#### 視覚化

```
  x₂
   ↑
 1 │ ●     ○   ← どんな直線でも
   │           ●と○を分離できない！
 0 │ ○     ●
   └─────────→ x₁
     0     1

● = y=1の点
○ = y=0の点
```

**問題点：**
- 対角線上の点が同じクラス
- 1本の直線では分離不可能
- **非線形分離問題**

---

### 3.3 数学的証明

**XORを満たす単層パーセプトロンは存在しない**

```
XORの条件：
  0×w₁ + 0×w₂ + b ≤ 0  →  b ≤ 0     ... (1)
  1×w₁ + 0×w₂ + b > 0  →  w₁ + b > 0 ... (2)
  0×w₁ + 1×w₂ + b > 0  →  w₂ + b > 0 ... (3)
  1×w₁ + 1×w₂ + b ≤ 0  →  w₁ + w₂ + b ≤ 0 ... (4)

(2)より: w₁ > -b
(3)より: w₂ > -b
(1)より: b ≤ 0 なので -b ≥ 0

したがって: w₁ > 0, w₂ > 0

しかし(4)より: w₁ + w₂ ≤ -b
(1)より: -b ≥ 0

したがって: w₁ + w₂ ≤ 0

矛盾！（w₁ > 0 かつ w₂ > 0 なら w₁ + w₂ > 0 のはず）
```

**結論：** 単層パーセプトロンではXORは実現不可能

---

## 4. 多層パーセプトロン

### 4.1 XORの解決策

**層を重ねることで非線形問題を解決**

#### 基本アイデア

```
XOR = (x₁ NAND x₂) AND (x₁ OR x₂)
```

**真理値表で確認：**
| x₁ | x₂ | s₁（NAND）| s₂（OR）| y（AND）| 正解 |
|----|----|-----------|---------|---------|----- |
| 0  | 0  | 1         | 0       | 0       | 0 ✓ |
| 1  | 0  | 1         | 1       | 1       | 1 ✓ |
| 0  | 1  | 1         | 1       | 1       | 1 ✓ |
| 1  | 1  | 0         | 1       | 0       | 0 ✓ |

---

### 4.2 ネットワーク構造

```
入力層    第1層（中間層）  第2層（出力層）
           NAND
x₁ ─────→   s₁   ─────┐
    ╲                  ├─→ AND → y（XOR）
     ╲    OR           ╱
      ╲→  s₂   ───────┘
x₂ ─────┘

層の構成：
- 入力層: 2ニューロン (x₁, x₂)
- 第1層:  2ニューロン (NAND, OR)
- 第2層:  1ニューロン (AND)
```

**計算の流れ：**
```
ステップ1: 入力 (x₁, x₂) を受け取る
ステップ2: 第1層で NAND と OR を計算
ステップ3: 第2層で AND を計算
ステップ4: 出力 y を得る
```

---

### 4.3 実装

```python
def XOR(x1, x2):
    # 第1層
    s1 = NAND(x1, x2)
    s2 = OR(x1, x2)
    
    # 第2層
    y = AND(s1, s2)
    return y

# 各ゲートの定義
def NAND(x1, x2):
    w1, w2, b = -0.5, -0.5, 0.7
    tmp = x1*w1 + x2*w2 + b
    return 1 if tmp > 0 else 0

def OR(x1, x2):
    w1, w2, b = 0.5, 0.5, -0.2
    tmp = x1*w1 + x2*w2 + b
    return 1 if tmp > 0 else 0

def AND(x1, x2):
    w1, w2, b = 0.5, 0.5, -0.7
    tmp = x1*w1 + x2*w2 + b
    return 1 if tmp > 0 else 0

# テスト
print(XOR(0, 0))  # 0
print(XOR(1, 0))  # 1
print(XOR(0, 1))  # 1
print(XOR(1, 1))  # 0
```

---

### 4.4 動作の詳細

**入力 (1, 0) の場合：**
```
ステップ1: x₁=1, x₂=0

ステップ2（第1層）:
  s₁ = NAND(1, 0)
     = 1×(-0.5) + 0×(-0.5) + 0.7
     = 0.2 > 0 → 1
  
  s₂ = OR(1, 0)
     = 1×0.5 + 0×0.5 + (-0.2)
     = 0.3 > 0 → 1

ステップ3（第2層）:
  y = AND(1, 1)
    = 1×0.5 + 1×0.5 + (-0.7)
    = 0.3 > 0 → 1

結果: XOR(1, 0) = 1 ✓
```

---

### 4.5 視覚的理解

```
2層のパーセプトロンが作る決定境界：

第1層（NAND, OR）:
  x₂              x₂
   ↑               ↑
 1 │●  ○ ←NAND  1 │●  ●  ←OR
 0 │○  ●         0 │○  ●
   └────→x₁        └────→x₁

第2層（AND）:
第1層の出力(s₁,s₂)を入力として処理
  s₂
   ↑
 1 │○  ●  ←AND
 0 │○  ○
   └────→s₁

最終的にXORの分類が実現される！
```

---

## 5. パーセプトロンの表現力

### 5.1 層を重ねるメリット

```
┌────────────────────────────────┐
│ 層の数 │ 表現できる問題        │
├────────┼───────────────────────┤
│ 単層   │ 線形分離可能な問題    │
│        │ (AND, OR, NAND)       │
├────────┼───────────────────────┤
│ 2層    │ 非線形問題            │
│        │ (XOR, 複雑な領域)     │
├────────┼───────────────────────┤
│ 多層   │ より複雑なパターン    │
│        │ (画像認識, 音声認識等)│
└────────────────────────────────┘
```

**理論的結果：**
- 2層のパーセプトロン：任意の領域を表現可能
- 3層のパーセプトロン：さらに効率的な表現

---

### 5.2 重要な洞察

```
層を重ねる = 表現力の向上

単層: ───────  直線的な分離
      ○●○

2層:  ───╱╲──  曲線的な分離
     ○●  ●○

多層: ─╱╲╱╲─  複雑な形状
    ○●○●○●
```

**ディープラーニングへの道：**
- パーセプトロン → 多層パーセプトロン
- 手動設定の重み → 学習による自動調整
- 論理回路 → 複雑なパターン認識

---

## 第2章のキーポイント

```
┌──────────────────────────────────────┐
│ パーセプトロンの重要概念              │
├──────────────────────────────────────┤
│ ✓ 重み：入力の重要度を制御           │
│ ✓ バイアス：発火しやすさを制御       │
│ ✓ 単層：線形分離可能な問題のみ       │
│ ✓ XOR：単層では実現不可能            │
│ ✓ 多層：非線形問題も解決可能         │
│ ✓ 層を重ねる→表現力が増大           │
└──────────────────────────────────────┘
```

---

## 復習問題

### Q1: パーセプトロンの基本

パーセプトロンで $w_1=0.5, w_2=0.5, b=-0.3$ のとき、入力 $(x_1, x_2) = (1, 1)$ の出力を計算せよ。

<details>
<summary>答えを見る</summary>

**答え:** y = 1

**計算過程：**
```
y = w₁x₁ + w₂x₂ + b
  = 0.5×1 + 0.5×1 + (-0.3)
  = 0.5 + 0.5 - 0.3
  = 0.7

0.7 > 0 なので y = 1
```

</details>

---

### Q2: NANDゲートの実装

NANDゲートを実装せよ。ただし、重みとバイアスの値を自分で設定すること。

<details>
<summary>答えを見る</summary>

**答え:**
```python
def NAND(x1, x2):
    w1, w2, b = -0.5, -0.5, 0.7
    tmp = x1*w1 + x2*w2 + b
    if tmp <= 0:
        return 0
    else:
        return 1
```

**検証：**
```python
print(NAND(0, 0))  # 1
print(NAND(1, 0))  # 1
print(NAND(0, 1))  # 1
print(NAND(1, 1))  # 0
```

**パラメータの設定理由：**
- 重みを負に → 入力が大きいほど出力が小さい
- バイアスを正に → デフォルトで1を出力しやすい
- (1, 1)のときだけ出力が0になるように調整

</details>

---

### Q3: 線形分離可能性

以下の点を1本の直線で分離できるか答えよ：

点A(0,0)=0, 点B(1,0)=1, 点C(0,1)=1, 点D(1,1)=0

<details>
<summary>答えを見る</summary>

**答え:** 分離できない

**理由：**
これはXORの問題であり、線形分離不可能。

```
  x₂
   ↑
 1 │ ●(1)  ○(0)  ← 対角線上の点が
   │               同じクラス
 0 │ ○(0)  ●(1)  ← 1本の直線では
   └─────────→ x₁  分離不可能
     0     1
```

**視覚的確認：**
どの直線を引いても、●と○を完全に分離することはできない。
- 横線：上下を分けるが、対角が問題
- 縦線：左右を分けるが、対角が問題
- 斜線：いずれも対角の点を分離できない

**解決策：** 多層パーセプトロンが必要

</details>

---

### Q4: XORの実装

多層パーセプトロンを使ってXORゲートを実装せよ。

<details>
<summary>答えを見る</summary>

**答え:**
```python
def XOR(x1, x2):
    # 第1層
    s1 = NAND(x1, x2)
    s2 = OR(x1, x2)
    
    # 第2層
    y = AND(s1, s2)
    return y

# 各ゲートの定義
def NAND(x1, x2):
    w1, w2, b = -0.5, -0.5, 0.7
    tmp = x1*w1 + x2*w2 + b
    return 1 if tmp > 0 else 0

def OR(x1, x2):
    w1, w2, b = 0.5, 0.5, -0.2
    tmp = x1*w1 + x2*w2 + b
    return 1 if tmp > 0 else 0

def AND(x1, x2):
    w1, w2, b = 0.5, 0.5, -0.7
    tmp = x1*w1 + x2*w2 + b
    return 1 if tmp > 0 else 0

# テスト
for x1 in [0, 1]:
    for x2 in [0, 1]:
        print(f"XOR({x1}, {x2}) = {XOR(x1, x2)}")
```

**出力：**
```
XOR(0, 0) = 0
XOR(1, 0) = 1
XOR(0, 1) = 1
XOR(1, 1) = 0
```

**ネットワーク構造：**
```
     NAND
x₁ ────→ s₁ ──┐
  ╲            ├─→ AND → y
   ╲→ OR       │
x₂ ────→ s₂ ──┘

2層構造により非線形問題を解決
```

</details>

---

### Q5: 重みとバイアスの役割

以下の2つのパーセプトロンの違いを説明せよ：
- (A) w₁=1.0, w₂=0.1, b=-0.5
- (B) w₁=0.5, w₂=0.5, b=-0.5

<details>
<summary>答えを見る</summary>

**答え:**

**(A) w₁=1.0, w₂=0.1, b=-0.5**
- x₁の重みが大きい（1.0）
- x₂の重みが小さい（0.1）
- **x₁の影響が支配的**

計算例：
```
(x₁,x₂) = (1,0): 1.0×1 + 0.1×0 - 0.5 = 0.5 > 0 → 1
(x₁,x₂) = (0,1): 1.0×0 + 0.1×1 - 0.5 = -0.4 ≤ 0 → 0
```
→ x₁が1なら出力1になりやすい

**(B) w₁=0.5, w₂=0.5, b=-0.5**
- x₁とx₂の重みが等しい（0.5）
- **両方の入力が同等に重要**

計算例：
```
(x₁,x₂) = (1,0): 0.5×1 + 0.5×0 - 0.5 = 0 ≤ 0 → 0
(x₁,x₂) = (0,1): 0.5×0 + 0.5×1 - 0.5 = 0 ≤ 0 → 0
(x₁,x₂) = (1,1): 0.5×1 + 0.5×1 - 0.5 = 0.5 > 0 → 1
```
→ 両方が1のときのみ出力1（ANDゲート）

**まとめ：**
- 重みの大きさ → 入力の重要度
- 重みのバランス → 各入力の相対的な影響力

</details>

---

## 学習チェックリスト

第2章の理解度を確認しましょう：

### 基本概念
- [ ] パーセプトロンの仕組みを説明できる
- [ ] 重みの役割を理解している
- [ ] バイアスの役割を理解している
- [ ] 閾値とバイアスの関係を説明できる

### 実装
- [ ] ANDゲートを実装できる
- [ ] NANDゲートを実装できる
- [ ] ORゲートを実装できる
- [ ] 適切な重みとバイアスを設定できる

### 理論
- [ ] 線形分離可能性を理解している
- [ ] XORが単層で実現できない理由を説明できる
- [ ] XORの数学的証明を理解している
- [ ] 多層化により表現力が増すことを理解している

### 応用
- [ ] XORを多層パーセプトロンで実装できる
- [ ] ネットワーク構造を図示できる
- [ ] 層を重ねる意義を説明できる

---

## 次章への準備

### 第3章で学ぶこと

**ニューラルネットワーク - 学習可能なモデルへ**

```
第2章: パーセプトロン
    ↓
手動で重みを設定
    ↓
第3章: ニューラルネットワーク
    ↓
活性化関数の導入
    ↓
連続的な出力
    ↓
学習アルゴリズムの適用が可能に
```

**第2章の知識がどう活かされるか：**
- パーセプトロン → ニューロンの基本単位
- 多層構造 → ニューラルネットワークの構造
- 重みとバイアス → 学習対象のパラメータ
- XOR問題 → 非線形活性化関数の必要性

---

## まとめ

### 第2章で学んだこと

```
┌────────────────────────────────────┐
│ パーセプトロンの本質               │
├────────────────────────────────────┤
│ 1. 重み付き入力の合計を計算        │
│ 2. 閾値と比較して出力を決定        │
│ 3. 単層では線形分離のみ可能        │
│ 4. 多層化で非線形問題も解決        │
└────────────────────────────────────┘
```

### 重要な洞察

**1. 重みとバイアスの役割**
- 重み：各入力の重要度
- バイアス：発火のしやすさ

**2. 単層の限界**
- 線形分離可能な問題のみ
- XORは実現不可能

**3. 多層化の威力**
- 非線形問題を解決
- 表現力の指数的増大

**4. ニューラルネットワークへの道**
- パーセプトロン：基本単位
- 多層化：複雑なパターン認識
- 学習：重みの自動調整（次章）

**パーセプトロンがニューラルネットワークの基礎となります！**

次章では、活性化関数を導入し、学習可能なニューラルネットワークを構築します 🚀
