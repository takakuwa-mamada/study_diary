# パーセプトロン（Perceptron）

## パーセプトロンとは

パーセプトロンは、**ニューラルネットワーク（深層学習）の起源**となるアルゴリズムです。1957年にFrank Rosenblattによって考案されました。

### 基本的な仕組み

- パーセプトロンは**複数の信号を受け取り、一つの信号を出力する**
- 入力信号には0または1の2値をとる
- 出力も0または1の2値

## 2入力パーセプトロンの数式

2つの入力信号を受け取るパーセプトロンは、以下の数式で表されます：

```
y = { 0  (w₁x₁ + w₂x₂ ≤ θ)
    { 1  (w₁x₁ + w₂x₂ > θ)
```

**記号の説明：**
- `x₁, x₂`：入力信号
- `w₁, w₂`：**重み（weight）** - 各入力信号の重要度を調整するパラメータ
- `θ`（シータ）：**閾値（threshold）** - 出力を決定する境界値
- `y`：出力信号

### 重みの意味

**重み（weight）は各入力信号の重要度を制御します。**
- 重みが大きいほど、その入力信号が結果に与える影響が大きい
- 重みが小さいと、その入力信号の影響は小さい
- 重みを調整することで、パーセプトロンの振る舞いを変えることができる

## バイアス（bias）を導入した表現

上記の式を、**バイアス b = -θ** として書き換えると：

```
y = { 0  (b + w₁x₁ + w₂x₂ ≤ 0)
    { 1  (b + w₁x₁ + w₂x₂ > 0)
```

**バイアスの役割：**
- バイアスは「発火のしやすさ」を調整するパラメータ
- バイアスが大きいほど、ニューロンが発火しやすくなる（出力が1になりやすい）
- 重みとバイアスを学習することで、パーセプトロンの性能を向上させる

## 論理回路の実装

パーセプトロンを使って、基本的な論理回路を実装できます：

### ANDゲート
両方の入力が1のときだけ出力が1

**パラメータ例：** `w₁=0.5, w₂=0.5, θ=0.7` または `w₁=0.5, w₂=0.5, b=-0.7`

### NANDゲート（NOT AND）
ANDゲートの出力を反転

**パラメータ例：** `w₁=-0.5, w₂=-0.5, b=0.7`

### ORゲート
いずれかの入力が1のとき出力が1

**パラメータ例：** `w₁=0.5, w₂=0.5, b=-0.2`

## パーセプトロンの限界

**単層パーセプトロンでは線形分離可能な問題しか解けません。**

### XORゲートの問題

XOR（排他的論理和）は、単層パーセプトロンでは実装できません：

| x₁ | x₂ | y |
|----|----|---|
| 0  | 0  | 0 |
| 1  | 0  | 1 |
| 0  | 1  | 1 |
| 1  | 1  | 0 |

XORは**非線形分離問題**であり、1本の直線では分離できません。

## 多層パーセプトロン

**解決策：パーセプトロンを多層に重ねる**

XORは、既存のゲートを組み合わせることで実装できます：

```
XOR = (NAND AND OR) の組み合わせ
s₁ = NAND(x₁, x₂)
s₂ = OR(x₁, x₂)
y = AND(s₁, s₂)
```

このように、**2層のパーセプトロンでXORを実装可能**です。

## データサイエンスにおける重要なポイント

### 1. 特徴量の重要度（重み）
データサイエンスでは、どの特徴量が予測に重要かを理解することが重要です。パーセプトロンの重みは、この**特徴量の重要度**を表現しています。

### 2. 線形分離可能性
多くの機械学習問題において、データが線形分離可能かどうかは重要な判断基準です。
- **線形分離可能**：単純なモデルで高精度が得られる
- **非線形分離**：より複雑なモデル（多層ニューラルネットワークなど）が必要

### 3. 層を重ねることの重要性
単層では表現できない複雑なパターンも、**層を重ねることで表現可能**になります。これが深層学習（ディープラーニング）の基本原理です。

### 4. パラメータの学習
実際の機械学習では、重みとバイアスを**データから自動的に学習**します（パーセプトロンの例では手動で設定しましたが）。この学習プロセスが「訓練（training）」です。

## まとめ

- パーセプトロンは複数の入力から一つの出力を生成する基本的なニューロンモデル
- 重みは各入力の重要度、バイアスは発火のしやすさを制御
- 単層パーセプトロンは線形分離可能な問題のみ解決可能
- 多層化することで、より複雑な非線形問題も解決可能
- これがニューラルネットワークと深層学習の基礎となる