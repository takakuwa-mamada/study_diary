# 第3章 総復習：ニューラルネットワーク

## 📚 この章の学習目標

**パーセプトロンを発展させ、データから学習できるネットワークを構築する**

```
パーセプトロン
    ↓
活性化関数の導入
    ↓
多層ニューラルネットワーク
    ↓
ソフトマックス関数
    ↓
MNIST実装
    ↓
実用的な画像認識
```

---

## 1. 活性化関数

### 1.1 活性化関数とは

**入力信号の加重和を出力信号に変換する関数**

```
ニューロンの計算プロセス：

入力 → 加重和 → 活性化 → 出力
x      a=Σwx+b   h(a)     y

1. 加重和：    a = b + w₁x₁ + w₂x₂ + ... + wₙxₙ
2. 活性化：    y = h(a)  ← h()が活性化関数
```

---

### 1.2 なぜ活性化関数が必要？

**非線形性を導入し、複雑なパターンを表現可能にする**

```
┌─────────────────────────────────┐
│ 線形関数のみの場合              │
├─────────────────────────────────┤
│ 何層重ねても1層と等価           │
│ → 複雑なパターンを表現不可能   │
└─────────────────────────────────┘

┌─────────────────────────────────┐
│ 非線形活性化関数を使用          │
├─────────────────────────────────┤
│ 層を重ねるごとに表現力が増大    │
│ → 複雑なパターンを学習可能     │
└─────────────────────────────────┘
```

**証明：線形関数では層を重ねる意味がない**
```
h(x) = cx とすると（線形関数）

3層のネットワーク：
y = h(h(h(x)))
  = h(h(c₁x))
  = h(c₂c₁x)
  = c₃c₂c₁x
  = ax  （ただし a = c₁c₂c₃）

→ 単なる1層の線形関数と等価
```

---

## 2. 主要な活性化関数

### 2.1 ステップ関数

#### 定義

$$
h(x) = \begin{cases}
0 & (x \leq 0) \\
1 & (x > 0)
\end{cases}
$$

#### 実装

```python
def step_function(x):
    return np.where(x > 0, 1, 0)
```

#### グラフ

```
y ↑
1 │         ━━━━━━  ← x>0で1
  │        ╱
0 │━━━━━━━╱         ← x≤0で0
  └───────────────→ x
     0
```

#### 特徴

```
┌─────────────────────────┐
│ 出力       │ 0または1    │
│ 連続性     │ 非連続      │
│ 微分可能性 │ 不可（x=0） │
│ 用途       │ パーセプトロン│
└─────────────────────────┘
```

---

### 2.2 シグモイド関数 ⭐

#### 定義

$$
h(x) = \frac{1}{1 + \exp(-x)}
$$

#### 実装

```python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
```

#### グラフ

```
y ↑
1 │       ─────
  │      ╱
0.5│    ╱        ← S字カーブ（滑らか）
  │   ╱
0 │──╱
  └───────────→ x
    -5  0  5
```

#### 特徴

```
┌─────────────────────────┐
│ 出力       │ 0〜1の実数  │
│ 連続性     │ 連続        │
│ 微分可能性 │ 可能 ✓      │
│ 用途       │ 2値分類     │
│ 欠点       │ 勾配消失問題 │
└─────────────────────────┘
```

#### ステップ関数との比較

```
ステップ関数      シグモイド関数
─────────────────────────────
2値出力(0,1)  →  実数出力(0〜1)
非連続        →  連続 ✓
微分不可      →  微分可能 ✓
学習困難      →  学習可能 ✓
```

**共通点：**
- 両方とも0〜1の範囲
- 入力が大きいほど出力も大きい（単調増加）

**相違点：**
- シグモイドは滑らかで連続
- シグモイドは微分可能（学習に必須）

---

### 2.3 ReLU関数（Rectified Linear Unit）⭐⭐⭐

#### 定義

$$
h(x) = \max(0, x) = \begin{cases}
0 & (x \leq 0) \\
x & (x > 0)
\end{cases}
$$

#### 実装

```python
def relu(x):
    return np.maximum(0, x)
```

#### グラフ

```
y ↑
5 │         ╱
4 │       ╱
3 │     ╱
2 │   ╱
1 │ ╱
0 ├─────────→ x
  -5  0  5

x<0: y=0（フラット）
x>0: y=x（傾き1）
```

#### 特徴

```
┌─────────────────────────┐
│ 出力       │ 0以上の実数 │
│ 連続性     │ 連続        │
│ 微分可能性 │ ほぼ可能 ✓  │
│ 計算       │ 非常に高速 ✓│
│ 用途       │ 隠れ層 ✓    │
│ 主流       │ 現代のDL ✓ │
└─────────────────────────┘
```

---

### 2.4 なぜReLUが主流なのか？⭐⭐⭐

#### 1. 計算効率

```python
# ReLU: 単純な比較（超高速）
y = max(0, x)

# シグモイド: 指数計算（重い）
y = 1 / (1 + exp(-x))

速度比：ReLUはシグモイドの約6倍速い
```

#### 2. 勾配消失問題の解決

```
シグモイドの問題：
σ(x) = 1/(1+exp(-x))
σ'(x) = σ(x)(1-σ(x))

x = ±5 のとき σ'(x) ≈ 0.007
→ 勾配が極めて小さい
→ 深い層で学習が停滞（勾配消失）

ReLUの利点：
ReLU'(x) = { 0  (x<0)
           { 1  (x≥0)

x>0 で常に勾配=1
→ 勾配が消失しない ✓
→ 深いネットワークでも学習可能 ✓
```

#### 3. スパース性（疎性）

```python
x = [-2, -1, 0, 1, 2, 3]
ReLU(x) = [0, 0, 0, 1, 2, 3]
          ↑  ↑  ↑
     50%が0（非活性化）

メリット：
- 計算量削減
- メモリ効率向上
- 過学習の抑制
- 生物学的妥当性
```

#### 4. 深いネットワークで安定

```
10層以上のネットワーク：
シグモイド: 勾配消失で学習困難
ReLU: 安定して学習可能 ✓

実績：
- ResNet（152層）
- VGG（19層）
- AlexNet
すべてReLUを使用
```

---

### 2.5 活性化関数の比較表

| 関数 | 式 | 出力範囲 | 微分可能 | 用途 | 特徴 |
|------|------|---------|---------|------|------|
| **ステップ** | $h(x)=\begin{cases}0\\1\end{cases}$ | {0, 1} | × | パーセプトロン | 2値、学習困難 |
| **シグモイド** | $\frac{1}{1+e^{-x}}$ | (0, 1) | ○ | 2値分類 | 勾配消失問題 |
| **ReLU** | $\max(0,x)$ | [0, ∞) | ○ | 隠れ層 | 高速、安定 ✓ |
| **tanh** | $\frac{e^x-e^{-x}}{e^x+e^{-x}}$ | (-1, 1) | ○ | 隠れ層 | シグモイドより良好 |

---

## 3. 多層ニューラルネットワークの実装

### 3.1 ネットワーク構造

```
入力層(2)  →  第1層(3)  →  第2層(2)  →  出力層(2)
   x₁         [ReLU]       [ReLU]        [恒等]
   x₂                                        y₁
                                              y₂

層の構成：
- 入力層:  2ニューロン
- 第1隠れ層: 3ニューロン（ReLU）
- 第2隠れ層: 2ニューロン（ReLU）
- 出力層:  2ニューロン（恒等関数）
```

---

### 3.2 実装

```python
import numpy as np

def relu(x):
    return np.maximum(0, x)

def identity_function(x):
    return x

# 入力
X = np.array([1.0, 0.5])

# 第1層（入力2 → 隠れ層3）
W1 = np.array([[0.1, 0.3, 0.5], 
               [0.2, 0.4, 0.6]])
B1 = np.array([0.1, 0.2, 0.3])

A1 = np.dot(X, W1) + B1  # 加重和
Z1 = relu(A1)             # 活性化

# 第2層（隠れ層3 → 隠れ層2）
W2 = np.array([[0.1, 0.4], 
               [0.2, 0.5], 
               [0.3, 0.6]])
B2 = np.array([0.1, 0.2])

A2 = np.dot(Z1, W2) + B2
Z2 = relu(A2)

# 第3層（隠れ層2 → 出力層2）
W3 = np.array([[0.1, 0.3], 
               [0.2, 0.4]])
B3 = np.array([0.1, 0.2])

A3 = np.dot(Z2, W3) + B3
Y = identity_function(A3)  # 恒等関数（回帰問題）

print(Y)
```

---

### 3.3 記号の意味

```
┌────────────────────────────────┐
│ 記号 │ 意味                    │
├──────┼────────────────────────┤
│ X    │ 入力                    │
│ W    │ 重み行列（Weight）      │
│ B    │ バイアス（Bias）        │
│ A    │ 活性化前の加重和        │
│ Z    │ 活性化後の出力          │
│ Y    │ 最終出力                │
└────────────────────────────────┘
```

**計算の流れ：**
```
X → [W1, B1] → A1 → ReLU → Z1
Z1 → [W2, B2] → A2 → ReLU → Z2
Z2 → [W3, B3] → A3 → 恒等 → Y
```

---

### 3.4 形状の確認

```python
X.shape   # (2,)      ← 入力: 2次元
W1.shape  # (2, 3)    ← 重み: 2×3
A1.shape  # (3,)      ← 加重和: 3次元
Z1.shape  # (3,)      ← 活性化後: 3次元

W2.shape  # (3, 2)    ← 重み: 3×2
A2.shape  # (2,)      ← 加重和: 2次元
Z2.shape  # (2,)      ← 活性化後: 2次元

W3.shape  # (2, 2)    ← 重み: 2×2
Y.shape   # (2,)      ← 出力: 2次元
```

**内積のルール：**
```
(m, n) × (n, k) = (m, k)

例：
(2,) × (2, 3) = (3,)
(3,) × (3, 2) = (2,)
(2,) × (2, 2) = (2,)
```

---

## 4. 出力層の設計

### 4.1 問題の種類による使い分け

```
┌──────────────────────────────────┐
│  問題タイプ  │  活性化関数      │
├──────────────┼──────────────────┤
│  回帰問題    │  恒等関数        │
│              │  （そのまま出力）│
├──────────────┼──────────────────┤
│  2値分類     │  シグモイド      │
│              │  （0〜1の確率） │
├──────────────┼──────────────────┤
│  多クラス分類│  ソフトマックス  │
│              │  （確率分布）    │
└──────────────────────────────────┘
```

---

### 4.2 恒等関数

**そのまま出力（回帰問題）**

```python
def identity_function(x):
    return x
```

**用途：**
- 株価予測
- 気温予測
- 連続値の予測

---

### 4.3 シグモイド関数

**0〜1の確率（2値分類）**

```python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
```

**用途：**
- スパム判定（スパム or 非スパム）
- 病気診断（陽性 or 陰性）
- 合否判定（合格 or 不合格）

---

## 5. ソフトマックス関数 ⭐⭐⭐

### 5.1 定義

$$
y_k = \frac{\exp(a_k)}{\sum_{i=1}^{n} \exp(a_i)}
$$

**意味：** 各クラスの「相対的な大きさ」を確率に変換

---

### 5.2 実装

```python
def softmax(a):
    c = np.max(a)  # オーバーフロー対策
    exp_a = np.exp(a - c)
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    return y
```

**使用例：**
```python
a = np.array([0.3, 2.9, 4.0])
y = softmax(a)
print(y)
# [0.018... 0.245... 0.735...]
print(np.sum(y))  # 1.0
```

---

### 5.3 特徴

#### 1. 出力の合計が1

```python
a = [1.0, 2.0, 3.0]
y = softmax(a)  # [0.09, 0.24, 0.67]
np.sum(y)  # 1.0

→ 確率として解釈可能
```

#### 2. 各要素は0〜1

```python
全ての y_i について：
0 < y_i < 1

→ 各クラスの確率
```

#### 3. 大きな値ほど大きな確率

```python
a = [1, 3, 2]
y = softmax(a)  # [0.09, 0.67, 0.24]
                #  ↑小   ↑大   ↑中

入力の大小関係が保持される
```

#### 4. 相対的な大小関係

```python
a = [1, 2, 3]
y1 = softmax(a)  # [0.09, 0.24, 0.67]

a = [11, 12, 13]  # 全体に+10
y2 = softmax(a)  # [0.09, 0.24, 0.67]

相対的な関係は変わらない
```

---

### 5.4 オーバーフロー対策

#### 問題

```python
# 指数関数は急激に増加
np.exp(10)    # 22026.4...
np.exp(100)   # 2.7 × 10^43
np.exp(1000)  # inf（無限大）

→ オーバーフロー
```

#### 解決策

```python
def softmax(a):
    c = np.max(a)  # 最大値を引く
    exp_a = np.exp(a - c)  # オーバーフロー回避
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    return y
```

**数学的に等価：**
$$
\frac{\exp(a_k + C)}{\sum \exp(a_i + C)} = \frac{\exp(a_k)}{\sum \exp(a_i)}
$$

$C = -\max(a)$ とすれば、最大値が0になり安全。

---

### 5.5 ソフトマックスの使い方

```
┌────────────────────────────────┐
│ 段階   │ ソフトマックスの使用 │
├────────┼───────────────────────┤
│ 訓練   │ 必要 ✓               │
│        │ （損失関数の計算）   │
├────────┼───────────────────────┤
│ 推論   │ 省略可               │
│        │ （argmaxだけでOK）   │
└────────────────────────────────┘
```

**理由：**
```python
# 大小関係は変わらない
a = [1, 2, 3]
np.argmax(a)  # 2

y = softmax(a)  # [0.09, 0.24, 0.67]
np.argmax(y)    # 2（同じ）

→ 推論時はsoftmax不要
```

---

## 6. MNISTデータセット

### 6.1 データセットの概要

**MNIST**：機械学習の「Hello World」

```
┌────────────────────────────┐
│ 訓練画像  │ 60,000枚       │
│ テスト画像│ 10,000枚       │
│ 画像サイズ│ 28×28ピクセル  │
│ 色        │ グレースケール │
│ ラベル    │ 0〜9の数字     │
└────────────────────────────┘
```

**画像の例：**
```
28×28 = 784ピクセル

  ████████
 ██      ██
██        ██
          ██   ← 数字の「5」
    ████████
  ██
 ██
██████████
```

---

### 6.2 データ読み込み

```python
from dataset.mnist import load_mnist

(x_train, t_train), (x_test, t_test) = \
    load_mnist(flatten=True, normalize=False)

print(x_train.shape)  # (60000, 784)
print(t_train.shape)  # (60000,)
print(x_test.shape)   # (10000, 784)
print(t_test.shape)   # (10000,)
```

**引数の意味：**
```
┌──────────────────────────────────┐
│ 引数          │ 説明              │
├───────────────┼──────────────────┤
│ normalize     │ 0.0〜1.0に正規化  │
│ flatten       │ 1次元配列に変換   │
│               │ (28,28)→(784,)   │
│ one_hot_label │ one-hot表現       │
│               │ 5→[0,0,0,0,0,1...│
└──────────────────────────────────┘
```

---

### 6.3 画像の表示

```python
import numpy as np
from PIL import Image

def img_show(img):
    pil_img = Image.fromarray(np.uint8(img))
    pil_img.show()

img = x_train[0]
label = t_train[0]
print(label)  # 例：5

img = img.reshape(28, 28)  # 784 → 28×28
img_show(img)
```

---

### 6.4 ニューラルネットワークで推論

```python
import numpy as np
import pickle
from dataset.mnist import load_mnist

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def softmax(a):
    c = np.max(a)
    exp_a = np.exp(a - c)
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    return y

def predict(network, x):
    W1, W2, W3 = network["W1"], network["W2"], network["W3"]
    b1, b2, b3 = network["b1"], network["b2"], network["b3"]
    
    # 第1層
    a1 = np.dot(x, W1) + b1
    z1 = sigmoid(a1)
    
    # 第2層
    a2 = np.dot(z1, W2) + b2
    z2 = sigmoid(a2)
    
    # 第3層（出力層）
    a3 = np.dot(z2, W3) + b3
    y = softmax(a3)
    
    return y

# データ読み込み
(x_train, t_train), (x_test, t_test) = \
    load_mnist(normalize=True, flatten=True)

# 事前学習済みモデル読み込み
with open("sample_weight.pkl", 'rb') as f:
    network = pickle.load(f)

# 推論
accuracy_cnt = 0
for i in range(len(x_test)):
    y = predict(network, x_test[i])
    p = np.argmax(y)  # 最も確率が高いクラス
    if p == t_test[i]:
        accuracy_cnt += 1

print(f"Accuracy: {float(accuracy_cnt) / len(x_test)}")
# 実行結果：Accuracy: 0.9352
```

**93.52%の精度**で手書き数字を認識！

---

### 6.5 バッチ処理

**1枚ずつ処理するのは非効率** → まとめて処理

```python
batch_size = 100

for i in range(0, len(x_test), batch_size):
    x_batch = x_test[i:i+batch_size]
    y_batch = predict(network, x_batch)
    p = np.argmax(y_batch, axis=1)  # 各行の最大値
    accuracy_cnt += np.sum(p == t_test[i:i+batch_size])
```

**メリット：**
- 計算の高速化（NumPyの内部最適化）
- 並列計算が可能
- メモリの効率化

**`axis=1`の意味：**
```python
y_batch.shape  # (100, 10)

np.argmax(y_batch, axis=1)
# 各データ（行）ごとに最大値のインデックス
# shape: (100,)
```

---

## 第3章のキーポイント

```
┌──────────────────────────────────────┐
│ ニューラルネットワークの重要概念      │
├──────────────────────────────────────┤
│ ✓ 活性化関数：非線形性の導入         │
│ ✓ ReLU：現代のDLで主流 ⭐⭐⭐        │
│ ✓ シグモイド：滑らかで微分可能       │
│ ✓ ソフトマックス：多クラス分類 ⭐    │
│ ✓ 多層化：複雑なパターンを学習       │
│ ✓ MNIST：実践的な画像認識            │
│ ✓ バッチ処理：効率的な計算           │
└──────────────────────────────────────┘
```

---

## 復習問題

### Q1: ReLU関数の出力

次の入力に対するReLU関数の出力を答えよ：

```python
x = np.array([-2, -1, 0, 1, 2])
y = relu(x)
```

<details>
<summary>答えを見る</summary>

**答え:**
```python
y = [0, 0, 0, 1, 2]
```

**解説:**
```
ReLU(x) = max(0, x)

x=-2 → max(0,-2) = 0
x=-1 → max(0,-1) = 0
x=0  → max(0,0)  = 0
x=1  → max(0,1)  = 1
x=2  → max(0,2)  = 2
```

**視覚的理解:**
```
入力: [-2, -1, 0, 1, 2]
      ↓   ↓  ↓  ↓  ↓
      0   0  0  そのまま
出力: [0, 0, 0, 1, 2]
```

</details>

---

### Q2: ソフトマックス関数の性質

以下のソフトマックス関数の出力について答えよ：

```python
a = np.array([1.0, 2.0, 3.0])
y = softmax(a)
```

(1) `y`の各要素の範囲は？
(2) `np.sum(y)`の値は？
(3) 最も確率が高いクラスは？

<details>
<summary>答えを見る</summary>

**答え:**

(1) **各要素の範囲：** 0 < y < 1（開区間）
```
y ≈ [0.090, 0.245, 0.665]
各要素は0より大きく、1より小さい
```

(2) **合計値：** `np.sum(y) = 1.0`
```
ソフトマックスの性質：
全要素の合計は必ず1
→ 確率分布として解釈可能
```

(3) **最も確率が高いクラス：** クラス2（インデックス2）
```python
np.argmax(y)  # 2

y = [0.090, 0.245, 0.665]
               ↑最大値
```

**ソフトマックスの意味:**
- 入力が大きい → 出力も大きい
- 全体で1に正規化 → 確率として解釈

</details>

---

### Q3: 活性化関数の使い分け

以下の問題に適した活性化関数を答えよ：

(1) 隠れ層
(2) 出力層（回帰問題）
(3) 出力層（2値分類）
(4) 出力層（多クラス分類）

<details>
<summary>答えを見る</summary>

**答え:**

(1) **隠れ層：** **ReLU** ⭐⭐⭐
```
理由：
- 計算が高速
- 勾配消失問題を回避
- 深いネットワークで安定
- 現代のDLで主流
```

(2) **出力層（回帰問題）：** **恒等関数**
```
理由：
- 実数値をそのまま出力
- 範囲制限なし

例：株価予測、気温予測
```

(3) **出力層（2値分類）：** **シグモイド関数**
```
理由：
- 0〜1の確率を出力
- 閾値0.5で分類

例：スパム判定、病気診断
```

(4) **出力層（多クラス分類）：** **ソフトマックス関数** ⭐
```
理由：
- 各クラスの確率分布
- 全体で1に正規化
- argmaxで予測

例：MNIST（0〜9の分類）
```

**まとめ:**
```
┌──────────────────────────────┐
│ 用途     │ 活性化関数        │
├──────────┼──────────────────┤
│ 隠れ層   │ ReLU ✓           │
│ 回帰     │ 恒等関数         │
│ 2値分類  │ シグモイド       │
│ 多クラス │ ソフトマックス ✓ │
└──────────────────────────────┘
```

</details>

---

### Q4: 線形関数では層を重ねる意味がない

活性化関数として線形関数 `h(x) = cx` を使った場合、3層のネットワークが1層と等価になることを証明せよ。

<details>
<summary>答えを見る</summary>

**答え:**

**証明：**

3層のネットワークで線形関数を使用した場合：

```
第1層: y₁ = c₁x
第2層: y₂ = c₂y₁ = c₂(c₁x)
第3層: y₃ = c₃y₂ = c₃(c₂(c₁x))

全体：
y₃ = c₃ × c₂ × c₁ × x
   = (c₁c₂c₃) × x
   = a × x  （ただし a = c₁c₂c₃）
```

これは単なる1層の線形関数 `y = ax` と等価。

**一般化：**
```
n層の線形関数：
y = cₙ × ... × c₂ × c₁ × x
  = (c₁c₂...cₙ) × x
  = Cx

→ 何層重ねても1層の線形変換
```

**結論：**
非線形活性化関数が必須。
- 線形：層を重ねても意味なし
- 非線形：層を重ねるごとに表現力が増大 ✓

</details>

---

### Q5: MNISTの推論

以下のコードの空欄を埋めて、MNISTの推論を完成させよ：

```python
def predict(network, x):
    W1, W2, W3 = network["W1"], network["W2"], network["W3"]
    b1, b2, b3 = network["b1"], network["b2"], network["b3"]
    
    # 第1層
    a1 = _____(x, W1) + b1
    z1 = _____(a1)
    
    # 第2層
    a2 = np.dot(z1, W2) + b2
    z2 = sigmoid(a2)
    
    # 第3層（出力層）
    a3 = np.dot(z2, W3) + b3
    y = _____(a3)
    
    return y
```

<details>
<summary>答えを見る</summary>

**答え:**

```python
def predict(network, x):
    W1, W2, W3 = network["W1"], network["W2"], network["W3"]
    b1, b2, b3 = network["b1"], network["b2"], network["b3"]
    
    # 第1層
    a1 = np.dot(x, W1) + b1  # 加重和
    z1 = sigmoid(a1)          # 活性化
    
    # 第2層
    a2 = np.dot(z1, W2) + b2
    z2 = sigmoid(a2)
    
    # 第3層（出力層）
    a3 = np.dot(z2, W3) + b3
    y = softmax(a3)           # 出力層：ソフトマックス
    
    return y
```

**ポイント:**
1. **np.dot()**: 内積（行列の積）
2. **sigmoid()**: 隠れ層の活性化関数
3. **softmax()**: 出力層（多クラス分類）

**ネットワーク構造:**
```
x (784,)
  ↓ W1(784,50), b1(50)
a1 (50) → sigmoid → z1 (50)
  ↓ W2(50,100), b2(100)
a2 (100) → sigmoid → z2 (100)
  ↓ W3(100,10), b3(10)
a3 (10) → softmax → y (10)
```

</details>

---

## 学習チェックリスト

第3章の理解度を確認しましょう：

### 活性化関数
- [ ] 活性化関数の役割を説明できる
- [ ] ステップ、シグモイド、ReLUの違いを理解している
- [ ] ReLUが主流である理由を3つ説明できる
- [ ] 線形関数では層を重ねる意味がないことを証明できる
- [ ] 用途に応じて活性化関数を選択できる

### ソフトマックス関数
- [ ] ソフトマックス関数の定義を説明できる
- [ ] オーバーフロー対策の必要性を理解している
- [ ] 出力が確率分布になることを説明できる
- [ ] 訓練時と推論時の使い分けを理解している

### ニューラルネットワーク
- [ ] 多層ニューラルネットワークを実装できる
- [ ] 重み行列とバイアスの形状を理解している
- [ ] 順伝播（forward propagation）を実装できる
- [ ] 内積のルール (m,n)×(n,k)=(m,k) を理解している

### MNIST
- [ ] MNISTデータセットを読み込める
- [ ] 画像データを表示できる
- [ ] 事前学習済みモデルで推論できる
- [ ] バッチ処理を実装できる
- [ ] argmaxで予測クラスを取得できる

---

## 次章への準備

### 第4章で学ぶこと

**ニューラルネットワークの学習 - パラメータの自動調整**

```
第3章: 推論（Inference）
    ↓
事前学習済みの重みを使用
    ↓
第4章: 学習（Training）
    ↓
データから重みを自動調整
    ↓
損失関数・勾配降下法
    ↓
誤差逆伝播法
```

**第3章の知識がどう活かされるか：**
- 活性化関数 → 微分可能な関数（学習に必須）
- 順伝播 → 予測値の計算
- ソフトマックス → 損失関数の計算
- MNIST → 実践的な学習データ

**学習のキーワード：**
- **損失関数（Loss Function）**：予測の誤差を測る
- **勾配降下法（Gradient Descent）**：最適化アルゴリズム
- **誤差逆伝播法（Backpropagation）**：効率的な勾配計算
- **ミニバッチ学習**：データをまとめて学習

---

## まとめ

### 第3章で習得したスキル

```
┌────────────────────────────────────┐
│ ニューラルネットワークの実装       │
├────────────────────────────────────┤
│ 活性化関数                         │
│  ├─ ステップ（パーセプトロン）     │
│  ├─ シグモイド（滑らか）           │
│  └─ ReLU（現代の主流）✓           │
│                                    │
│ 出力層の設計                       │
│  ├─ 恒等関数（回帰）               │
│  ├─ シグモイド（2値分類）          │
│  └─ ソフトマックス（多クラス）✓   │
│                                    │
│ 実装スキル                         │
│  ├─ 多層ネットワークの構築         │
│  ├─ 順伝播の実装                   │
│  ├─ MNIST推論（93.52%の精度）     │
│  └─ バッチ処理                     │
└────────────────────────────────────┘
```

### 重要な概念

1. **非線形性の重要性**
   - 線形関数では表現力が限定的
   - 非線形活性化関数で複雑なパターンを学習

2. **ReLUの優位性**
   - 計算効率
   - 勾配消失問題の解決
   - 深いネットワークで安定

3. **ソフトマックス関数**
   - 多クラス分類で必須
   - 確率分布として解釈可能
   - オーバーフロー対策が重要

4. **実践的な実装**
   - 形状管理が重要
   - バッチ処理で効率化
   - NumPyの活用

**これでニューラルネットワークの推論が実装できるようになりました！**

次章では、データから重みを自動的に学習する方法を習得します 🚀
