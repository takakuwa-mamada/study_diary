# 第1章〜第3章 総復習：ディープラーニングの基礎

## 📚 全体の学習マップ

```
第1章: Python入門        第2章: パーセプトロン    第3章: ニューラルネットワーク
    ↓                        ↓                          ↓
NumPy/Matplotlib      単層→多層パーセプトロン    活性化関数・学習の実装
    ↓                        ↓                          ↓
   配列演算の基礎       非線形問題の解決         実用的なネットワーク
                              ↓
                    ディープラーニングの基盤が完成
```

---

## 第1章：Python入門

### 📖 この章の要点

**ディープラーニング実装に必要なPythonの基礎スキルを習得**

---

### 1. NumPy - 数値計算の基盤

#### NumPyとは

**NumPy**は、Pythonで高速な数値計算を行うためのライブラリです。

**なぜ必要？**
- Pythonの標準リストは数値計算が遅い
- NumPyはC言語で実装され、非常に高速
- ディープラーニングの計算は行列演算が中心

#### NumPy配列の作成

```python
import numpy as np

# 配列の作成
x = np.array([1, 2, 3])
print(type(x))  # <class 'numpy.ndarray'>

# ndarray = N-dimensional array（N次元配列）
```

#### 算術演算（要素ごと）

```python
x = np.array([1, 2, 3])
y = np.array([4, 5, 6])

print(x + y)  # [5 7 9]   ← 要素ごとの加算
print(x * y)  # [4 10 18] ← 要素ごとの乗算
print(x / y)  # [0.25 0.4 0.5]
```

**視覚的理解：**
```
x + y の計算：
  [1, 2, 3]
+ [4, 5, 6]
-----------
  [5, 7, 9]  ← 各要素を対応する位置で計算
```

#### N次元配列

```python
# 2次元配列（行列）
A = np.array([[1, 2, 3], 
              [4, 5, 6]])

print(A.shape)  # (2, 3) → 2行3列
print(A.ndim)   # 2 → 2次元
```

**形状（shape）の意味：**
```
shape = (2, 3)
         ↑  ↑
         行 列

  列0 列1 列2
行0 [ 1   2   3 ]
行1 [ 4   5   6 ]
```

#### ブロードキャスト ⭐⭐⭐

**異なる形状の配列同士を自動的に拡張して計算**

```python
A = np.array([[1, 2], 
              [3, 4]])
B = np.array([10, 20])

print(A * B)
# [[10 40]
#  [30 80]]
```

**何が起こっている？**
```
A.shape = (2, 2)
B.shape = (2,)

NumPyが自動でBを拡張：
B = [10, 20]
  ↓ ブロードキャスト
B = [[10, 20],  ← 各行に複製
     [10, 20]]

計算：
  [[1  2]     [[10 20]     [[10 40]
   [3  4]]  ×  [10 20]]  =  [30 80]]
```

**重要性：**
- ループ不要で効率的
- コードが簡潔
- ディープラーニングで頻繁に使用

---

### 2. Matplotlib - データ可視化

```python
import matplotlib.pyplot as plt

# グラフの描画
x = np.arange(0, 6, 0.1)
y1 = np.sin(x)
y2 = np.cos(x)

plt.plot(x, y1, label='sin')
plt.plot(x, y2, linestyle='--', label='cos')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.show()
```

**データ可視化の重要性：**
- データの分布・傾向を直感的に把握
- 学習の進行状況を監視
- 結果の説明・プレゼンテーション

---

### 第1章のキーポイント

```
┌─────────────────────────────────────┐
│ NumPyの重要概念                      │
├─────────────────────────────────────┤
│ ✓ ndarray（N次元配列）              │
│ ✓ 要素ごとの演算                    │
│ ✓ 形状（shape）の理解               │
│ ✓ ブロードキャスト                  │
│ ✓ ベクトル化（ループ不要の高速計算）│
└─────────────────────────────────────┘
```

---

## 第2章：パーセプトロン

### 📖 この章の要点

**ニューラルネットワークの起源となるアルゴリズムを理解**

---

### 1. パーセプトロンの基本

#### 仕組み

```
入力    重み      ニューロン
x₁ ───→ w₁ ──┐
              ├─→ Σ → [閾値判定] → y（出力）
x₂ ───→ w₂ ──┘
```

**複数の信号を受け取り、一つの信号を出力**

#### 数式

$$
y = \begin{cases}
0 & (w_1x_1 + w_2x_2 \leq \theta) \\
1 & (w_1x_1 + w_2x_2 > \theta)
\end{cases}
$$

**記号の意味：**
- $x_1, x_2$：入力信号（0 または 1）
- $w_1, w_2$：**重み** - 入力の重要度
- $\theta$：**閾値** - 出力の境界値
- $y$：出力信号（0 または 1）

---

### 2. バイアスの導入

#### バイアス表記

$$
y = \begin{cases}
0 & (b + w_1x_1 + w_2x_2 \leq 0) \\
1 & (b + w_1x_1 + w_2x_2 > 0)
\end{cases}
$$

ここで $b = -\theta$（バイアス）

#### 重みとバイアスの役割

```
┌─────────────────────────────────┐
│ パラメータ │ 役割              │
├───────────┼───────────────────┤
│ 重み (w)  │ 入力の重要度を制御 │
│ バイアス(b)│ 発火しやすさを制御│
└─────────────────────────────────┘
```

**バイアスの意味：**
- バイアスが大きい（正）→ 発火しやすい
- バイアスが小さい（負）→ 発火しにくい

---

### 3. 論理回路の実装

#### ANDゲート

```python
def AND(x1, x2):
    w1, w2, b = 0.5, 0.5, -0.7
    tmp = x1*w1 + x2*w2 + b
    if tmp <= 0:
        return 0
    else:
        return 1
```

**真理値表：**
| x₁ | x₂ | y |
|----|----|---|
| 0  | 0  | 0 |
| 1  | 0  | 0 |
| 0  | 1  | 0 |
| 1  | 1  | 1 |

#### NANDゲート

```python
def NAND(x1, x2):
    w1, w2, b = -0.5, -0.5, 0.7
    tmp = x1*w1 + x2*w2 + b
    if tmp <= 0:
        return 0
    else:
        return 1
```

#### ORゲート

```python
def OR(x1, x2):
    w1, w2, b = 0.5, 0.5, -0.2
    tmp = x1*w1 + x2*w2 + b
    if tmp <= 0:
        return 0
    else:
        return 1
```

---

### 4. 単層パーセプトロンの限界

#### 線形分離可能性

**単層パーセプトロンは1本の直線で分離できる問題のみ解決可能**

```
ANDゲート（線形分離可能）：
  x₂
   ↑
 1 │ ○     ●
   │  ＼
 0 │ ○  ＼  ○   ← 直線で分離可能
   └─────────→ x₁
     0     1
```

#### XORゲートの問題 ⭐

**真理値表：**
| x₁ | x₂ | y（XOR）|
|----|----|---------|
| 0  | 0  | 0       |
| 1  | 0  | 1       |
| 0  | 1  | 1       |
| 1  | 1  | 0       |

**視覚化：**
```
  x₂
   ↑
 1 │ ●     ○   ← どんな直線でも
   │           ●と○を分離できない
 0 │ ○     ●
   └─────────→ x₁
     0     1

● = y=1の点
○ = y=0の点
```

**数学的証明：**

XORを満たす重みとバイアスは存在しない：
```
条件：
  0×w₁ + 0×w₂ + b ≤ 0  →  b ≤ 0     ... (1)
  1×w₁ + 0×w₂ + b > 0  →  w₁ + b > 0 ... (2)
  0×w₁ + 1×w₂ + b > 0  →  w₂ + b > 0 ... (3)
  1×w₁ + 1×w₂ + b ≤ 0  →  w₁ + w₂ + b ≤ 0 ... (4)

(2)と(3)より: w₁ > 0, w₂ > 0
(4)より: w₁ + w₂ ≤ -b
(1)より: -b ≥ 0

矛盾！（w₁>0 かつ w₂>0 なら w₁+w₂>0 のはず）
```

---

### 5. 多層パーセプトロン

#### XORの実装

**解決策：層を重ねる**

```
XOR = (x₁ NAND x₂) AND (x₁ OR x₂)
```

**ネットワーク構造：**
```
入力層    第1層（中間層）  第2層（出力層）
           NAND
x₁ ─────→   s₁   ─────┐
    ╲                  ├─→ AND → y（XOR）
     ╲    OR           ╱
      ╲→  s₂   ───────┘
x₂ ─────┘
```

**真理値表で確認：**
| x₁ | x₂ | s₁（NAND）| s₂（OR）| y（AND）| 正解 |
|----|----|-----------|---------|---------|----- |
| 0  | 0  | 1         | 0       | 0       | 0 ✓ |
| 1  | 0  | 1         | 1       | 1       | 1 ✓ |
| 0  | 1  | 1         | 1       | 1       | 1 ✓ |
| 1  | 1  | 0         | 1       | 0       | 0 ✓ |

**実装：**
```python
def XOR(x1, x2):
    s1 = NAND(x1, x2)  # 第1層
    s2 = OR(x1, x2)    # 第1層
    y = AND(s1, s2)    # 第2層
    return y
```

---

### 第2章のキーポイント

```
┌──────────────────────────────────────┐
│ パーセプトロンの重要概念              │
├──────────────────────────────────────┤
│ ✓ 重み：入力の重要度を制御           │
│ ✓ バイアス：発火しやすさを制御       │
│ ✓ 単層：線形分離可能な問題のみ       │
│ ✓ 多層：非線形問題も解決可能         │
│ ✓ 層を重ねる→表現力が増大           │
└──────────────────────────────────────┘
```

---

## 第3章：ニューラルネットワーク

### 📖 この章の要点

**パーセプトロンを発展させ、データから学習できるネットワークを構築**

---

### 1. 活性化関数

#### 活性化関数とは

**入力信号の加重和を出力信号に変換する関数**

```
計算プロセス：
1. 加重和：    a = b + w₁x₁ + w₂x₂ + ... + wₙxₙ
2. 活性化：    y = h(a)  ← h()が活性化関数
```

#### なぜ必要？

**非線形性を導入し、複雑なパターンを表現可能にする**

---

### 2. ステップ関数

#### 定義

$$
h(x) = \begin{cases}
0 & (x \leq 0) \\
1 & (x > 0)
\end{cases}
$$

#### 実装

```python
def step_function(x):
    return np.where(x > 0, 1, 0)
```

#### グラフ

```
y ↑
1 │         ━━━━━━  ← x>0で1
  │        ╱
0 │━━━━━━━╱         ← x≤0で0
  └───────────────→ x
     0
```

**特徴：**
- 0または1の2値のみ
- 非連続
- 微分不可能（0の点で）

---

### 3. シグモイド関数 ⭐

#### 定義

$$
h(x) = \frac{1}{1 + \exp(-x)}
$$

#### 実装

```python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
```

#### グラフ

```
y ↑
1 │       ─────
  │      ╱
0.5│    ╱        ← S字カーブ
  │   ╱
0 │──╱
  └───────────→ x
    -5  0  5
```

**特徴：**
- 0〜1の実数値を出力
- 滑らかなS字カーブ
- 連続かつ微分可能
- 非線形関数

**シグモイドの重要性：**
```
ステップ関数との比較：

ステップ関数      シグモイド関数
─────────────────────────────
2値出力(0,1)  →  実数出力(0〜1)
非連続        →  連続
微分不可      →  微分可能 ✓
学習困難      →  学習可能 ✓
```

---

### 4. ReLU関数（Rectified Linear Unit）⭐⭐⭐

#### 定義

$$
h(x) = \max(0, x) = \begin{cases}
0 & (x \leq 0) \\
x & (x > 0)
\end{cases}
$$

#### 実装

```python
def relu(x):
    return np.maximum(0, x)
```

#### グラフ

```
y ↑
5 │         ╱
4 │       ╱
3 │     ╱
2 │   ╱
1 │ ╱
0 ├─────────→ x
  -5  0  5

x<0: y=0（フラット）
x>0: y=x（傾き1）
```

**特徴：**
- 計算が非常にシンプル
- 勾配消失問題を緩和
- 深いネットワークで安定
- **現代のDLで最も主流**

#### なぜReLUが主流？

**1. 計算効率**
```python
# ReLU: 単純な比較
y = max(0, x)

# シグモイド: 指数計算（重い）
y = 1 / (1 + exp(-x))
```

**2. 勾配消失問題の解決**
```
シグモイド：
  x=±5で勾配≈0 → 学習が停滞

ReLU：
  x>0で勾配=1 → 学習が安定
```

**3. スパース性**
```python
x = [-2, -1, 0, 1, 2]
y = ReLU(x)
# y = [0, 0, 0, 1, 2]
#      ↑  ↑  ↑  50%が非活性化

利点：
- メモリ効率良い
- 過学習抑制
```

---

### 5. 活性化関数の比較

| 関数 | 出力範囲 | 微分可能 | 用途 | 特徴 |
|------|---------|---------|------|------|
| **ステップ** | {0, 1} | × | パーセプトロン | 2値、学習困難 |
| **シグモイド** | (0, 1) | ○ | 2値分類 | 勾配消失問題 |
| **ReLU** | [0, ∞) | ○ | 隠れ層 | 高速、安定 ✓ |
| **tanh** | (-1, 1) | ○ | 隠れ層 | シグモイドより良好 |

---

### 6. 多層ニューラルネットワークの実装

#### ネットワーク構造

```
入力層(2)  →  第1層(3)  →  第2層(2)  →  出力層(2)
   x₁                                        y₁
   x₂                                        y₂
```

#### 実装

```python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# 入力
X = np.array([1.0, 0.5])

# 第1層
W1 = np.array([[0.1, 0.3, 0.5], 
               [0.2, 0.4, 0.6]])
B1 = np.array([0.1, 0.2, 0.3])

A1 = np.dot(X, W1) + B1  # 加重和
Z1 = sigmoid(A1)          # 活性化

# 第2層
W2 = np.array([[0.1, 0.4], 
               [0.2, 0.5], 
               [0.3, 0.6]])
B2 = np.array([0.1, 0.2])

A2 = np.dot(Z1, W2) + B2
Z2 = sigmoid(A2)

# 第3層（出力層）
W3 = np.array([[0.1, 0.3], 
               [0.2, 0.4]])
B3 = np.array([0.1, 0.2])

A3 = np.dot(Z2, W3) + B3
Y = A3  # 恒等関数（回帰問題）

print(Y)
```

#### 記号の意味

```
X: 入力
W: 重み行列（Weight）
B: バイアス（Bias）
A: 活性化前の加重和
Z: 活性化後の出力
Y: 最終出力
```

#### 形状の確認

```python
X.shape   # (2,)
W1.shape  # (2, 3)
A1.shape  # (3,)
Z1.shape  # (3,)
W2.shape  # (3, 2)
A2.shape  # (2,)
Z2.shape  # (2,)
W3.shape  # (2, 2)
Y.shape   # (2,)
```

**内積のルール：** $(m, n) \times (n, k) = (m, k)$

---

### 7. 出力層の設計

#### 問題の種類による使い分け

```
┌──────────────────────────────────┐
│  問題タイプ  │  活性化関数      │
├──────────────┼──────────────────┤
│  回帰問題    │  恒等関数        │
│              │  （そのまま出力）│
├──────────────┼──────────────────┤
│  2値分類     │  シグモイド      │
│              │  （0〜1の確率） │
├──────────────┼──────────────────┤
│  多クラス分類│  ソフトマックス  │
│              │  （確率分布）    │
└──────────────────────────────────┘
```

---

### 8. ソフトマックス関数 ⭐

#### 定義

$$
y_k = \frac{\exp(a_k)}{\sum_{i=1}^{n} \exp(a_i)}
$$

**意味：** 各クラスの「相対的な大きさ」を確率に変換

#### 実装

```python
def softmax(a):
    c = np.max(a)  # オーバーフロー対策
    exp_a = np.exp(a - c)
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    return y

# 使用例
a = np.array([0.3, 2.9, 4.0])
y = softmax(a)
print(y)
# [0.018... 0.245... 0.735...]
print(np.sum(y))  # 1.0
```

#### 特徴

1. **出力の合計が1** → 確率として解釈可能
2. **各要素は0〜1** → 各クラスの確率
3. **大きな値ほど大きな確率**
4. **相対的な大小関係を保持**

#### オーバーフロー対策

```python
# 問題
np.exp(1000)  # inf（無限大）

# 解決策：最大値を引く
c = np.max(a)
np.exp(a - c)  # 安全に計算可能
```

**数学的に等価：**
$$
\frac{\exp(a_k + C)}{\sum \exp(a_i + C)} = \frac{\exp(a_k)}{\sum \exp(a_i)}
$$

---

### 9. MNISTデータセット

#### データセットの概要

**MNIST**：機械学習の「Hello World」

```
訓練画像: 60,000枚
テスト画像: 10,000枚
画像サイズ: 28×28ピクセル（グレースケール）
ラベル: 0〜9の数字
```

#### データ読み込み

```python
from dataset.mnist import load_mnist

(x_train, t_train), (x_test, t_test) = \
    load_mnist(flatten=True, normalize=False)

print(x_train.shape)  # (60000, 784)
print(t_train.shape)  # (60000,)
print(x_test.shape)   # (10000, 784)
print(t_test.shape)   # (10000,)
```

**引数の意味：**
- `normalize`：0.0〜1.0に正規化
- `flatten`：1次元配列に変換（28×28 → 784）
- `one_hot_label`：one-hot表現

#### 画像の表示

```python
from PIL import Image

def img_show(img):
    pil_img = Image.fromarray(np.uint8(img))
    pil_img.show()

img = x_train[0]
label = t_train[0]
print(label)  # 例：5

img = img.reshape(28, 28)  # 784 → 28×28
img_show(img)
```

---

### 第3章のキーポイント

```
┌──────────────────────────────────────┐
│ ニューラルネットワークの重要概念      │
├──────────────────────────────────────┤
│ ✓ 活性化関数：非線形性の導入         │
│ ✓ ReLU：現代のDLで主流               │
│ ✓ ソフトマックス：多クラス分類       │
│ ✓ 多層化：複雑なパターンを学習       │
│ ✓ MNIST：実践的な画像認識            │
└──────────────────────────────────────┘
```

---

## 章間の関連性

### 知識の積み上げ

```
第1章: NumPy/Matplotlib
   ↓ 配列演算・可視化の基礎
第2章: パーセプトロン
   ↓ ニューロンモデル・多層化の概念
第3章: ニューラルネットワーク
   ↓ 活性化関数・実用的な実装
第4章: 学習アルゴリズム（次章）
```

### 各章の役割

**第1章：** ツールの習得
- NumPy：高速計算
- Matplotlib：結果の可視化

**第2章：** 概念の理解
- パーセプトロン：基本単位
- 多層化：表現力の向上

**第3章：** 実装の習得
- 活性化関数：学習可能に
- ネットワーク：実用的な構造

---

## 総合復習問題

### 基礎問題

#### Q1: NumPyに関する問題

以下のコードの出力を答えよ：

```python
import numpy as np
A = np.array([[1, 2], [3, 4]])
B = np.array([10, 20])
print(A * B)
```

<details>
<summary>答えを見る</summary>

**答え:**
```python
[[10 40]
 [30 80]]
```

**解説:**
ブロードキャストにより、Bが各行に拡張される：
```
B = [10, 20]
  ↓
B = [[10, 20],
     [10, 20]]

A * B = [[1×10, 2×20],
         [3×10, 4×20]]
      = [[10, 40],
         [30, 80]]
```

</details>

---

#### Q2: パーセプトロンに関する問題

NANDゲートの重みとバイアスを答えよ。ただし、ANDゲートは `w₁=0.5, w₂=0.5, b=-0.7` とする。

<details>
<summary>答えを見る</summary>

**答え:**
```
w₁ = -0.5
w₂ = -0.5
b = 0.7
```

**解説:**
NANDはANDの出力を反転する：
- ANDの重みを負にする（重要度を反転）
- バイアスの符号を反転させる

真理値表で確認：
```
x₁  x₂  AND  NAND
0   0    0    1
1   0    0    1
0   1    0    1
1   1    1    0
```

</details>

---

#### Q3: 活性化関数に関する問題

次の入力に対するReLU関数の出力を答えよ：

```python
x = np.array([-2, -1, 0, 1, 2])
y = relu(x)
```

<details>
<summary>答えを見る</summary>

**答え:**
```python
y = [0, 0, 0, 1, 2]
```

**解説:**
ReLUの定義：
```
ReLU(x) = max(0, x)

x=-2 → max(0,-2) = 0
x=-1 → max(0,-1) = 0
x=0  → max(0,0)  = 0
x=1  → max(0,1)  = 1
x=2  → max(0,2)  = 2
```

</details>

---

### 応用問題

#### Q4: XORゲートの実装

多層パーセプトロンを使ってXORゲートを実装せよ。NAND、OR、ANDゲートを組み合わせて実現すること。

<details>
<summary>答えを見る</summary>

**答え:**

```python
def XOR(x1, x2):
    # 第1層
    s1 = NAND(x1, x2)
    s2 = OR(x1, x2)
    
    # 第2層
    y = AND(s1, s2)
    return y

# 各ゲートの定義
def NAND(x1, x2):
    w1, w2, b = -0.5, -0.5, 0.7
    tmp = x1*w1 + x2*w2 + b
    return 1 if tmp > 0 else 0

def OR(x1, x2):
    w1, w2, b = 0.5, 0.5, -0.2
    tmp = x1*w1 + x2*w2 + b
    return 1 if tmp > 0 else 0

def AND(x1, x2):
    w1, w2, b = 0.5, 0.5, -0.7
    tmp = x1*w1 + x2*w2 + b
    return 1 if tmp > 0 else 0
```

**真理値表:**
| x₁ | x₂ | s₁(NAND) | s₂(OR) | y(AND) | XOR |
|----|----|----------|--------|--------|-----|
| 0  | 0  | 1        | 0      | 0      | 0 ✓ |
| 1  | 0  | 1        | 1      | 1      | 1 ✓ |
| 0  | 1  | 1        | 1      | 1      | 1 ✓ |
| 1  | 1  | 0        | 1      | 0      | 0 ✓ |

</details>

---

#### Q5: ソフトマックス関数の性質

以下のソフトマックス関数の出力について答えよ：

```python
a = np.array([1.0, 2.0, 3.0])
y = softmax(a)
```

(1) `y`の各要素の範囲は？
(2) `np.sum(y)`の値は？
(3) なぜオーバーフロー対策が必要か？

<details>
<summary>答えを見る</summary>

**答え:**

(1) **各要素の範囲：** 0 < y < 1（開区間）
```
y ≈ [0.090, 0.245, 0.665]
各要素は0より大きく、1より小さい
```

(2) **合計値：** `np.sum(y) = 1.0`
```
ソフトマックスの定義により、
全要素の合計は必ず1になる
→ 確率分布として解釈可能
```

(3) **オーバーフロー対策の必要性：**
```python
# 問題：大きな値の指数
np.exp(1000)  # inf（無限大）

# 指数関数は急激に増加
exp(10) ≈ 22,026
exp(100) ≈ 2.7 × 10^43
exp(1000) → オーバーフロー

# 解決策：最大値を引く
a_max = np.max(a)
exp(a - a_max)  # 最大値が0になり安全
```

**数学的証明：**
$$
\frac{\exp(a_k)}{\sum \exp(a_i)} = \frac{C \cdot \exp(a_k)}{C \cdot \sum \exp(a_i)} = \frac{\exp(a_k + \log C)}{\sum \exp(a_i + \log C)}
$$

$C = \exp(-\max(a))$ とすれば、最大値が0になる。

</details>

---

### 実装問題

#### Q6: 3層ニューラルネットワークの実装

以下の仕様でニューラルネットワークを実装せよ：

- 入力層：2ニューロン
- 第1層（隠れ層）：3ニューロン（ReLU）
- 第2層（隠れ層）：2ニューロン（ReLU）
- 出力層：2ニューロン（恒等関数）

```python
# 入力
X = np.array([1.0, 0.5])

# 重みとバイアスは適当な値を設定
```

<details>
<summary>答えを見る</summary>

**答え:**

```python
import numpy as np

def relu(x):
    return np.maximum(0, x)

def identity_function(x):
    return x

# 入力
X = np.array([1.0, 0.5])

# 第1層（入力2 → 隠れ層3）
W1 = np.array([[0.1, 0.3, 0.5], 
               [0.2, 0.4, 0.6]])
B1 = np.array([0.1, 0.2, 0.3])

A1 = np.dot(X, W1) + B1
Z1 = relu(A1)

print("第1層出力:", Z1)
print("第1層形状:", Z1.shape)  # (3,)

# 第2層（隠れ層3 → 隠れ層2）
W2 = np.array([[0.1, 0.4], 
               [0.2, 0.5], 
               [0.3, 0.6]])
B2 = np.array([0.1, 0.2])

A2 = np.dot(Z1, W2) + B2
Z2 = relu(A2)

print("第2層出力:", Z2)
print("第2層形状:", Z2.shape)  # (2,)

# 第3層（隠れ層2 → 出力層2）
W3 = np.array([[0.1, 0.3], 
               [0.2, 0.4]])
B3 = np.array([0.1, 0.2])

A3 = np.dot(Z2, W3) + B3
Y = identity_function(A3)

print("最終出力:", Y)
print("出力形状:", Y.shape)  # (2,)
```

**形状の確認：**
```
X:  (2,)
W1: (2,3) → Z1: (3,)
W2: (3,2) → Z2: (2,)
W3: (2,2) → Y:  (2,)

内積のルール：
(m,n) × (n,k) = (m,k)
```

</details>

---

### 概念理解問題

#### Q7: なぜ非線形活性化関数が必要か？

線形関数 `h(x) = cx` を活性化関数として使った場合、何層重ねても意味がないことを証明せよ。

<details>
<summary>答えを見る</summary>

**答え:**

**証明：**

3層のネットワークで線形関数を使用した場合：

```
第1層: h(x) = c₁x
第2層: h(x) = c₂x
第3層: h(x) = c₃x

全体の出力:
y = c₃ × (c₂ × (c₁ × x))
  = c₃ × c₂ × c₁ × x
  = (c₁c₂c₃) × x
  = a × x  （ただし a = c₁c₂c₃）
```

これは単なる1層の線形関数 `y = ax` と等価。

**結論：**
線形関数を何層重ねても、結局1層の線形関数に帰着する。
→ **非線形活性化関数が必須**

**非線形関数のメリット：**
1. 層を重ねることで表現力が指数的に増大
2. 複雑なパターンを学習可能
3. XORのような非線形分離問題を解決

</details>

---

#### Q8: ReLUがシグモイドより優れている理由

ReLU関数がシグモイド関数より深層学習で好まれる理由を3つ挙げよ。

<details>
<summary>答えを見る</summary>

**答え:**

**1. 計算効率が高い**
```python
# ReLU: 比較のみ（高速）
y = max(0, x)

# シグモイド: 指数計算（低速）
y = 1 / (1 + exp(-x))

速度比：ReLUはシグモイドの約6倍速い
```

**2. 勾配消失問題の緩和**
```
シグモイドの問題：
σ(x) = 1/(1+exp(-x))
σ'(x) = σ(x)(1-σ(x))

x = ±5 のとき σ'(x) ≈ 0
→ 勾配が消失し、学習が停滞

ReLUの利点：
ReLU'(x) = { 0  (x<0)
           { 1  (x≥0)

x>0 で常に勾配=1
→ 勾配が消失しない
```

**3. スパース性（疎性）**
```python
x = [-2, -1, 0, 1, 2, 3]
ReLU(x) = [0, 0, 0, 1, 2, 3]
          ↑  ↑  ↑
     50%が0（非活性化）

メリット：
- 計算量削減
- メモリ効率向上
- 過学習の抑制
- 生物学的妥当性
```

**追加の理由：**
- 深いネットワーク（10層以上）で安定
- 学習が速く収束
- 実装がシンプル

</details>

---

#### Q9: ソフトマックス関数の役割

ソフトマックス関数を使う理由と、その数学的性質を説明せよ。

<details>
<summary>答えを見る</summary>

**答え:**

**使う理由：**

1. **確率分布への変換**
```
入力: [2.0, 1.0, 0.5]（任意の実数）
  ↓ ソフトマックス
出力: [0.659, 0.242, 0.099]（合計=1.0）

→ 各クラスの確率として解釈可能
```

2. **相対的な大小関係の保持**
```
入力が大きい → 出力も大きい確率
入力: [3, 2, 1]
出力: [0.665, 0.245, 0.090]
      ↑最大   ↑2番   ↑最小
順序が保持される
```

**数学的性質：**

1. **正規化:** $\sum_{i=1}^{n} y_i = 1$
```
全出力の合計が必ず1
→ 確率分布として妥当
```

2. **正値性:** $0 < y_i < 1$ for all $i$
```
各出力は必ず0より大きく1より小さい
→ 確率の定義に合致
```

3. **微分可能性:**
```
∂y_i/∂a_j が計算可能
→ 誤差逆伝播法で学習可能
```

4. **単調増加性:**
```
a_i が増加すると y_i も増加
→ 直感的に理解しやすい
```

**具体例：**
```python
# 3クラス分類
a = [2.0, 1.0, 0.5]
y = softmax(a)
# y = [0.659, 0.242, 0.099]

解釈：
クラス0: 65.9%の確率
クラス1: 24.2%の確率
クラス2: 9.9%の確率

予測: np.argmax(y) = 0（クラス0）
```

**推論時の注意：**
```
推論（予測）時は、argmaxだけで十分
ソフトマックスは省略可能
（大小関係は変わらないため）

訓練時は、損失関数の計算に必要
（交差エントロピー誤差など）
```

</details>

---

## 学習チェックリスト

第1章〜第3章の理解度を確認しましょう：

### 第1章：Python入門
- [ ] NumPy配列の作成と演算ができる
- [ ] ブロードキャストの概念を理解している
- [ ] 配列の形状（shape）を意識できる
- [ ] Matplotlibでグラフを描画できる
- [ ] ベクトル化の重要性を理解している

### 第2章：パーセプトロン
- [ ] パーセプトロンの仕組みを説明できる
- [ ] 重みとバイアスの役割を理解している
- [ ] AND、NAND、ORゲートを実装できる
- [ ] XORが単層で実現できない理由を説明できる
- [ ] 多層化により表現力が増すことを理解している

### 第3章：ニューラルネットワーク
- [ ] 活性化関数の役割を説明できる
- [ ] ステップ、シグモイド、ReLUの違いを理解している
- [ ] ReLUが主流である理由を説明できる
- [ ] 多層ニューラルネットワークを実装できる
- [ ] ソフトマックス関数の性質を理解している
- [ ] MNISTデータセットを扱える

---

## 次章への準備

### 第4章で学ぶこと

```
学習アルゴリズム
  ↓
重みとバイアスの自動調整
```

**キーワード：**
- 損失関数
- 勾配降下法
- 誤差逆伝播法
- ミニバッチ学習

**これまでの知識がどう使われるか：**
```
第1章 NumPy      → 高速な行列演算
第2章 パーセプトロン → ネットワークの基本構造
第3章 活性化関数   → 学習可能な非線形モデル
     ↓
第4章 学習アルゴリズム
     ↓
データから自動的にパラメータを最適化
```

---

## まとめ

### 第1章〜第3章で学んだこと

```
┌────────────────────────────────────────┐
│ 第1章: ツールの習得                     │
│   NumPy, Matplotlib                    │
│   配列演算・ブロードキャスト            │
├────────────────────────────────────────┤
│ 第2章: 基本概念の理解                   │
│   パーセプトロン・重みとバイアス        │
│   単層の限界・多層化の重要性            │
├────────────────────────────────────────┤
│ 第3章: 実装スキルの獲得                 │
│   活性化関数・ReLUの重要性              │
│   多層ネットワーク・ソフトマックス      │
└────────────────────────────────────────┘
```

### 3つの章の統合

**ディープラーニングの基盤が完成**

1. **計算ツール**（第1章）
   - NumPyで効率的な行列演算

2. **ネットワーク構造**（第2章）
   - 層を重ねて表現力を向上

3. **学習可能なモデル**（第3章）
   - 活性化関数で非線形パターンを学習

**次のステップ：**
第4章で学ぶ「学習アルゴリズム」により、これらを統合して**データから自動的に学習するシステム**を構築します。

---

**総復習完了！次は第4章「ニューラルネットワークの学習」へ進みましょう 🚀**
